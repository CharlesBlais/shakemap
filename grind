#!/usr/bin/env python

import sys
import os.path
import time as time
import argparse
from importlib import import_module
import copy

# Not really needed, but handy for looking at the config
import pprint

import numpy as np
import numexpr as ne

from openquake.hazardlib import imt
import openquake.hazardlib.const as oqconst
import openquake.hazardlib.gsim.base as oqbase

from shakelib.grind.rupture import PointRupture
from shakelib.grind.sites import Sites
from shakelib.grind.distance import Distance, get_distance
from shakelib.grind.multigmpe import MultiGMPE
from shakelib.grind.virtualipe import VirtualIPE

from shakelib.grind.container import InputContainer, OutputContainer
from shakemap.utils.config import get_config_paths

#%%
#
# helper function for things (ipe, gmice, ccf) that don't have a 
# fromConfig() constructor yet. Instantiates an instance of a 
# class from config entry, 'name', that has a corresponding 
# 'name'_module dictionary of class name, module path
#
def get_object_from_config(obj, cfg, *args):
    cls_abbr = cfg['grind'][obj]
    mods = obj + '_modules'
    (cname, mpath) = cfg[mods][cls_abbr]
    return getattr(import_module(mpath), cname)(*args)

#
# Set the attributes of the site context
#        
def set_sx_params(sx, lons, lats, vs30):      
    sx.lons            = lons
    sx.lats            = lats
    sx.vs30            = vs30
    sx.z1pt0_cy14_cal  = Sites._z1pt0_from_vs30_cy14_cal(sx.vs30)
    sx.z1pt0_ask14_cal = Sites._z1pt0_from_vs30_ask14_cal(sx.vs30)
    sx.z2pt5_cb14_cal  = Sites._z2pt5_from_vs30_cb14_cal(sx.vs30) / 1000.0
    sx.z1pt0_cy08      = Sites._z1pt0_from_vs30_cy08(sx.vs30)
    sx.z2pt5_cb07      = Sites._z2pt5_from_z1pt0_cb07(sx.z1pt0_cy08)
    sx.vs30measured    = np.zeros_like(lons)
    sx.backarc         = np.zeros_like(lons)
    return sx

#
# Helper function to call get_mean_and_stddevs for the 
# appropriate object given the IMT
#
def gmas(ipe, gmpe, sx, rx, dx, oqimt, stddev_types):
    if 'MMI' in oqimt:
        pe = ipe
    else:
        pe = gmpe
    return pe.get_mean_and_stddevs(sx, rx, dx, oqimt, stddev_types)

#: Earth radius in km.
EARTH_RADIUS = 6371.0
#
# Quick and dirty distance calculator; uses numexpr for speed
#
def geodetic_distance(lons1, lats1, lons2, lats2, diameter=2*EARTH_RADIUS):
    d = ne.evaluate("EARTH_RADIUS * sqrt(((lons1 - lons2) * cos(0.5 * " \
                    "(lats1 + lats2)))**2.0 + (lats1 - lats2)**2.0)")
    return d

def get_imts(imtstr, imtset):

    if imtstr in imtset:
        return (imtstr, )

    salist = [x for x in imtset if x.startswith('SA(')]
    periodlist = [float(x.replace('SA(', '').replace(')', '')) for x in salist]
    periodlist = sorted(periodlist)
    periodlist_str = [str(x) for x in periodlist]
    
    #
    # If we're here, then we know that IMT isn't in the inputs. Try
    # some alternatives.
    #
    if imtstr == 'PGA':
        #
        # Use the highest frequency in the inputs, otherwise use PGV
        #
        if len(salist):
            return ('SA(' + periodlist_str[0] + ')', )
        elif 'PGV' in imtset:
            return ('PGV', )
        else:
            return ()
    elif imtstr == 'PGV':
        #
        # Use 1.0 sec SA (or its bracket) if it's there, otherwise
        # use PGA
        #
        sa_tuple = get_sa_bracket(1.0, periodlist, periodlist_str)
        if sa_tuple != ():
            return sa_tuple
        if 'PGA' in imtset:
            return ('PGA', )
        else:
            return ()            
    elif imtstr == 'MMI':
        #
        # Use PGV if it's there, otherwise use 1.0 sec SA (or its
        # bracket)
        #
        if 'PGV' in imtset:
            return ('PGV', )
        return get_sa_bracket(1.0, periodlist, periodlist_str)
    elif imtstr.startswith('SA('):
        myper = float(imtstr.replace('SA(', '').replace(')', ''))
        return get_sa_bracket(myper, periodlist, periodlist_str)
    else:
        raise ValueError('Unknown IMT %s in get_imt_bracket' % imtstr)
        
def get_sa_bracket(myper, plist, plist_str):
        
    if not len(plist):
        return ()
    try:
        return ('SA(' + plist_str[plist.index(myper)] + ')', )
    except ValueError:
        pass
    for i, v in enumerate(plist):
        if v > myper:
            break
    if i == 0 or v < myper:
        return ('SA(' + plist_str[i] + ')', )
    else:
        return ('SA(' + plist_str[i-1] + ')', 'SA(' + plist_str[i] + ')')

#%%
args = type('Dummy', (object,), {'eventid' : 'northridge',
                                 'verbose' : True})
if True:
#def grind(args):
    verbose = args.verbose
    #
    # Find the shake_data file
    #
    install_path, data_path = get_config_paths()
    datadir = os.path.join(data_path, args.eventid)
    if not os.path.isdir(datadir):
        print('%s is not a valid directory.' % datadir)
        sys.exit(1)
    datafile = os.path.join(datadir, 'shake_data.hdf')
    if not os.path.isfile(datafile):
        print('%s is not a valid shake data file.' % datafile)
        sys.exit(1)
    #------------------------------------------------------------------
    # Make the input container and extract the config
    #------------------------------------------------------------------
    ic = InputContainer.loadFromHDF(datafile)
    config = ic.getConfig()
#    pprint.pprint(config)
    #------------------------------------------------------------------
    # Instantiate the gmpe, gmice, ipe, and ccf
    #------------------------------------------------------------------
    gmpe = MultiGMPE.from_config(config)

    gmice = get_object_from_config('gmice', config)

    if config['ipe_modules'][config['grind']['ipe']][0] == 'VirtualIPE':
        ipe = VirtualIPE.fromFuncs(gmpe, gmice)
    else:
        ipe = get_object_from_config('ipe', config)

#    ccf = get_object_from_config('ccf', config)
    #------------------------------------------------------------------
    # Bias parameters
    #------------------------------------------------------------------
    do_bias         = config['grind']['bias']['do_bias']
    bias_max_range  = config['grind']['bias']['max_range']
    bias_max_mag    = config['grind']['bias']['max_mag']
    bias_max_dsigma = config['grind']['bias']['max_delta_sigma']
    #------------------------------------------------------------------
    # Outlier parameters
    #------------------------------------------------------------------
    outlier_deviation_level = config['grind']['outlier']['max_deviation']
    outlier_max_mag = config['grind']['outlier']['max_mag']
    #------------------------------------------------------------------
    # These are the IMTs we want to make
    #------------------------------------------------------------------
    imt_out_set_str = set(config['grind']['imt_list'])
    imt_out_set_str.add('SA(2.0)')
    imt_out_set = [imt.from_string(x) for x in imt_out_set_str]
    #------------------------------------------------------------------
    # Get the rupture object and rupture context
    #------------------------------------------------------------------
    rupture_obj = ic.getRupture()
    rx = rupture_obj.getRuptureContext([gmpe])
    if rx.rake == None:
        rx.rake = 0
    #------------------------------------------------------------------
    # Get the Vs30 file name
    #------------------------------------------------------------------
    vs30default = config['grind']['vs30default']
    vs30_file = config['grind']['vs30file']
    if not vs30_file:
        vs30_file = None 
    #------------------------------------------------------------------
    # The output locations: either a grid or a list of points
    #------------------------------------------------------------------
    if config['grind']['prediction_location']['file']:
        #
        # FILE: Open the file and get the output points
        #
        lons, lats, idents = np.genfromtxt(
                        config['grind']['prediction_location']['file'],
                        autostrip=True, unpack=True)
        depths = np.zeros_like(lats)
        smnx = np.size(lons)
        smny = 1
        dist_obj_out = Distance(gmpe, lons, lats, depths, rupture_obj)
        #
        # Get the Vs30 from a file (could add other params here)
        # In the future we may want to support selecting
        # the Vs30 values from a grid
        #
        vs30_rock = np.full_like(lons, vs30default)
        if vs30_file:
            ids, vs = np.genfromtxt(vs30_file, autostrip=True, unpack=True)
            vs30_hash = dict(zip(ids, vs))
            vs30 = np.ndarray([vs30_hash[x] for x in idents])
        else:
            vs30 = vs30_rock
        sx_out_soil = oqbase.SitesContext()
        sx_out_rock = oqbase.SitesContext()
        
        sx_out_soil = set_sx_params(sx_out_soil, lons, lats, vs30)
        sx_out_rock = set_sx_params(sx_out_rock, lons, lats, vs30_rock)
    else:
        #
        # GRID: Figure out the grid parameters and get output points
        #
        smdx = config['grind']['prediction_location']['xres']
        smdy = config['grind']['prediction_location']['yres']

        W, S, E, N = config['grind']['prediction_location']['extent']
        
        sites_obj_out = Sites.fromBounds(W, E, S, N, smdx, smdy, 
                                         defaultVs30=vs30default, 
                                         vs30File=vs30_file)
        smnx, smny = sites_obj_out.getNxNy()
        
        sx_out_soil = sites_obj_out.getSitesContext()
        sx_out_rock = sites_obj_out.getSitesContext(rock_vs30=vs30default)
        lons, lats = np.meshgrid(sx_out_rock.lons, sx_out_rock.lats)
        lons = np.flipud(lons)
        lats = np.flipud(lats)
        lons = lons.flatten()
        lats = lats.flatten()
        depths = np.zeros_like(lats)

        dist_obj_out = Distance.fromSites(gmpe, sites_obj_out, rupture_obj)
    
    dx_out = dist_obj_out.getDistanceContext()
        
    lons_out_rad = np.radians(lons)
    lats_out_rad = np.radians(lats)
    #------------------------------------------------------------------
    # Station data
    #------------------------------------------------------------------
    stations = ic.getStationList()
    stddev_types = [oqconst.StdDev.TOTAL, oqconst.StdDev.INTER_EVENT, 
                    oqconst.StdDev.INTRA_EVENT]
    #
    # df1 holds the instrumented data (PGA, PGV, SA)
    # df2 holds the non-instrumented data (MMI)
    #
    df_dict = {'df1': None, 'df2': None}
    imt_in_str_dict = {'df1': None, 'df2': None}
    imt_in_dict = {'df1': None, 'df2': None}
    imt_in_str_set = set()
    sx_dict = {'df1': None, 'df2': None}
    dx_dict = {'df1': None, 'df2': None}
    if stations is not None:
        df_dict['df1'] = stations.getStationDataframe(1)
        df_dict['df2'] = stations.getStationDataframe(0)
        #
        # Get the sites and distance contexts for each dataframe then
        # compute the predictions for the IMTs in that dataframe.
        #
        for ndf, df in df_dict.items():
            if not df:
                continue
            #
            # Get lists of the input IMTs
            #
            imt_in_str_dict[ndf] = set(
                    [x for x in df.keys() if x in ('PGA', 'PGV', 'MMI') or 
                     x.startswith('SA(')])
            imt_in_dict[ndf] = set(
                    [imt.from_string(x) for x in imt_in_str_dict[ndf]])
            imt_in_str_set |= imt_in_str_dict[ndf]
            #
            # Get the sites and distance contexts
            #
            df['depth'] = np.zeros_like(df['lon'])
            lldict = {'lons': df['lon'], 'lats': df['lat']}
            sx_dict[ndf] = sites_obj_out.getSitesContext(lldict)
            dist_obj = Distance(gmpe, df['lon'], df['lat'], df['depth'], 
                                rupture_obj)
            dx_dict[ndf] = dist_obj.getDistanceContext()
            #
            # Do the predictions and other bookkeeping for each IMT
            #
            for imtstr in imt_in_str_dict[ndf]:
                pmean, psd = gmas(ipe, gmpe, sx_dict[ndf], rx, dx_dict[ndf], 
                                  imt.from_string(imtstr), stddev_types)
                df[imtstr + '_pred'] = pmean
                df[imtstr + '_pred_sd_total'] = psd[0]
                df[imtstr + '_pred_sd_inter'] = psd[1]
                df[imtstr + '_pred_sd_intra'] = psd[2]
                #
                # Compute the total residual
                #
                df[imtstr + '_residual'] = df[imtstr] - df[imtstr + '_pred']
                #
                # Do the outlier flagging if we don't have a fault and
                # the event magnitude is over the limit
                #
                if not isinstance(rupture_obj, PointRupture) or \
                   rx.mag <= outlier_max_mag:
                    #
                    # turn off nan warnings for this statement
                    #
                    np.seterr(invalid='ignore')
                    flagged = np.abs(df[imtstr + '_residual']) > \
                              outlier_deviation_level * \
                              df[imtstr + '_pred_sd_total']
                    np.seterr(invalid='warn')
                    if verbose:
                        print('IMT: %s, flagged: %d' % 
                              (imtstr, np.sum(flagged)))
                    df[imtstr + '_outliers'] = flagged
                    df[imtstr][flagged] = np.nan
                    df[imtstr + '_residual'][flagged] = np.nan
                #
                # Make the uncertainty arrays for any IMTs that don't
                # have them.
                #
                if (imtstr + '_sd') not in df:
                    if imtstr == 'MMI':
                        sdval = 0.3
                    else:
                        sdval = 0.0
                    df[imtstr + '_sd'] = np.full_like(df['lon'], sdval)
            #
            # Get the lons/lats in radians while we're at it
            #
            df['lon_rad'] = np.radians(df['lon'])
            df['lat_rad'] = np.radians(df['lat'])
            #
            # It will be handy later on to have the rupture distance
            # in the dataframes
            #
            dd = get_distance(['rrup'], df['lat'], df['lon'], 
                                      df['depth'], rupture_obj)
            df['rrup'] = dd['rrup']
    df1 = df_dict['df1']
    df2 = df_dict['df2']

#%%
    #------------------------------------------------------------------
    # Compute all the IMTs possible from MMI
    # This logic needs to be revisited. We should probably make what
    # we have to to do the CMS to make the needed output IMTs, but
    # for now, we're just going to use what we have and the ccf.
    #------------------------------------------------------------------
    if df2:
        for gmice_imt in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            if imt.SA == gmice_imt:
                iterlist = gmice.DEFINED_FOR_SA_PERIODS                    
            else:
                iterlist = [None]
            for period in iterlist:
                oqimt = gmice_imt(period)
                imtstr = str(oqimt)
                df2[imtstr], _ = gmice.getGMfromMI(df2['MMI'], oqimt, 
                                                   dists=df2['rrup'], 
                                                   mag=rx.mag)
                np.seterr(invalid='ignore')
                df2[imtstr][df2['MMI'] < 4.0] = np.nan
                np.seterr(invalid='warn')
                df2[imtstr + '_sd'] = np.full_like(df2['MMI'],
                       gmice.getMI2GMsd()[oqimt])
                imt_in_str_dict['df2'].add(imtstr)
                #
                # Get the predictions and stddevs, too
                #
#
# Should we bias the predictions?
#
                pmean, psd = gmas(ipe, gmpe, sx_dict['df2'], rx, 
                                  dx_dict['df2'], oqimt, stddev_types)
                df2[imtstr + '_pred'] = pmean
                df2[imtstr + '_pred_sd_total'] = psd[0]
                df2[imtstr + '_pred_sd_inter'] = psd[1]
                df2[imtstr + '_pred_sd_intra'] = psd[2]
                df2[imtstr + '_residual'] = df2[str(oqimt)] - pmean
                
    #
    # Now make derived MMI from the best available PGM; This is ugly and it
    # would be nice to have a more deterministic way of doing it
    #
    if df1:
        imtstr = None
        if 'PGV' in df1 \
                and imt.PGV in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            imtstr = 'PGV'
        elif 'PGA' in df1 \
                and imt.PGA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            imtstr = 'PGA'
        elif 'SA(1.0)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 1.0 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(1.0)'
        elif 'SA(0.3)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 0.3 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(0.3)'
        elif 'SA(3.0)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 3.0 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(3.0)'
        
        if imtstr is not None:
            oqimt = imt.from_string(imtstr)
            df1['MMI'], _ = gmice.getMIfromGM(df1[imtstr], oqimt,
                                              dists=df1['rrup'], mag=rx.mag)
            df1['MMI_sd'] = np.full_like(df1[imtstr], 
                                         gmice.getGM2MIsd()[oqimt])
            imt_in_str_dict['df1'].add('MMI')
            #
            # Get the prediction and stddevs
            #
            pmean, psd = gmas(ipe, gmpe, sx_dict['df1'], rx, dx_dict['df1'], 
                              imt.from_string('MMI'), stddev_types)
            df1['MMI' + '_pred'] = pmean
            df1['MMI' + '_pred_sd_total'] = psd[0]
            df1['MMI' + '_pred_sd_inter'] = psd[1]
            df1['MMI' + '_pred_sd_intra'] = psd[2]
            df1['MMI' + '_residual'] = df1['MMI'] - pmean


    imt_per = set()
    for ndf, sdf in df_dict.items():
        for imtstr in imt_in_str_dict[ndf]:
            if imtstr == 'PGA':
                imt_per.add(0.01)
            elif imtstr == 'PGV' or imtstr == 'MMI':
                imt_per.add(1.0)
            else:
                imt_per.add(imt.from_string(imtstr).period)
    for imtstr in imt_out_set_str:
        if imtstr == 'PGA':
            imt_per.add(0.01)
        elif imtstr == 'PGV' or imtstr == 'MMI':
            imt_per.add(1.0)
        else:
            imt_per.add(imt.from_string(imtstr).period)
        
    ccf = get_object_from_config('ccf', config, imt_per)

    imt_per_ix = {}
    for ix, per in enumerate(sorted(imt_per)):
        imt_per_ix[str(per)] = ix          
#%%
    #------------------------------------------------------------------
    # Do the MVN
    #------------------------------------------------------------------
    #
    # First do the bias for all of the input and output IMTs. Hold on
    # to some of the products that will be used for the interpolation.
    #
    bias = {}
    sigma_2_eta = {}
    outgrid = {}
    outsd = {}
    
    sta_lons_rad = {}
    sta_lats_rad = {}
    sta_resids = {}
    sta_period_ix = {}
    sta_sigs = {}
    corr_adj = {}
    sigma22inv = {}
    combined_imt_str = imt_in_str_dict['df1'] | imt_in_str_dict['df2'] \
                                              | imt_out_set_str
    for imtstr in combined_imt_str:
        #
        # Get the (pesudo-) period of the output IMT
        #
        time1 = time.time()
        oqimt = imt.from_string(imtstr)
        if imtstr == 'PGA':
            outperiod_ix = imt_per_ix['0.01']
        elif imtstr in ('PGV', 'MMI'):
            outperiod_ix = imt_per_ix['1.0']
        else:
            outperiod_ix = imt_per_ix[str(oqimt.period)]
        #
        # Get the predictions at the output points
        #
        pout_mean, pout_sd = gmas(ipe, gmpe, sx_out_soil, rx, dx_out, oqimt, 
                                  stddev_types)
        pout_sd = pout_sd[2]
        pout_sd2 = np.power(pout_sd, 2.0)

        ampgrid = np.zeros_like(pout_mean)
        sdgrid = np.zeros_like(pout_mean)
        
        sta_lons_rad[imtstr] = np.array([])
        sta_lats_rad[imtstr] = np.array([])
        sta_resids[imtstr] = np.array([])
        sta_sigs[imtstr] = np.array([])
        sta_sigs_total = np.array([])
        sta_period_ix[imtstr] = np.array([], dtype=np.int64)
        for ndf, sdf in df_dict.items():
            for imtin in get_imts(imtstr, imt_in_str_dict[ndf]):
                print('doing imt %s, input %s' % (imtstr, imtin))
                oqimtin = imt.from_string(imtin)
                if imtin == 'PGA':
                    inperiod_ix = imt_per_ix['0.01']
                elif imtin in ('PGV', 'MMI'):
                    inperiod_ix = imt_per_ix['1.0']
                else:
                    inperiod_ix = imt_per_ix[str(oqimtin.period)]
                vidx = ~np.isnan(sdf[imtin])
                sta_lons_rad[imtstr] = np.append(sta_lons_rad[imtstr], 
                                                 sdf['lon_rad'][vidx])
                sta_lats_rad[imtstr] = np.append(sta_lats_rad[imtstr], 
                                                 sdf['lat_rad'][vidx])
                sta_resids[imtstr] = np.append(sta_resids[imtstr], 
                                       sdf[imtin + '_residual'][vidx])
                psd = sdf[imtin + '_pred_sd_intra'][vidx]
                sta_sigs[imtstr] = np.append(sta_sigs[imtstr], psd)
                sta_sigs_total = np.append(sta_sigs_total, np.sqrt(
                        psd**2 + sdf[imtin + '_sd'][vidx]**2))
                sta_period_ix[imtstr] = np.append(sta_period_ix[imtstr], 
                                          np.full(psd.shape, inperiod_ix, 
                                                  dtype=np.int64))
        if np.size(sta_lons_rad) == 0:
            outgrid[imtstr] = pout_mean
            outsd[imtstr] = pout_sd[0]
            continue
        sta_lons_rad[imtstr] = sta_lons_rad[imtstr].reshape((-1, 1))
        sta_lats_rad[imtstr] = sta_lats_rad[imtstr].reshape((-1, 1))
        sta_resids[imtstr] = sta_resids[imtstr].reshape((-1, 1))
        sta_sigs[imtstr] = sta_sigs[imtstr].reshape((-1, 1))
        sta_sigs_total = sta_sigs_total.reshape((-1, 1))
        sta_period_ix[imtstr] = sta_period_ix[imtstr].reshape((-1, 1))
        corr_adj[imtstr] = (sta_sigs[imtstr] / sta_sigs_total)
        corr_adj22 = corr_adj[imtstr] * corr_adj[imtstr].T
        np.fill_diagonal(corr_adj22, 1.0)
        dist22 = geodetic_distance(sta_lons_rad[imtstr], 
                                   sta_lats_rad[imtstr],
                                   sta_lons_rad[imtstr].T, 
                                   sta_lats_rad[imtstr].T)
        d22_rows, d22_cols = np.shape(dist22) # should be square
        t1_22 = np.tile(sta_period_ix[imtstr], (1, d22_cols))
        t2_22 = np.tile(sta_period_ix[imtstr].T, (d22_rows, 1))
        corr22 = ccf.getCorrelation(t1_22, t2_22, dist22) * corr_adj22
        sigma22 = corr22 * (sta_sigs[imtstr] * sta_sigs[imtstr].T)
        sigma22inv[imtstr] = np.linalg.pinv(sigma22)
        #
        # Compute the bias and apply it to the predictions and the
        # residuals
        #
        if do_bias and (not isinstance(rupture_obj, PointRupture) 
                        or rx.mag <= bias_max_mag):        
            ONE = np.ones_like(sta_resids[imtstr])
            tau = np.mean(pout_sd[1])
            sigma_2_eta[imtstr] = 1.0 / (1 / tau**2 + \
                                   ONE.T.dot(sigma22inv[imtstr].dot(ONE)))
            bias[imtstr] = \
                    ONE.T.dot(sigma22inv[imtstr].dot(sta_resids[imtstr])) * \
                    sigma_2_eta[imtstr]
            if bias[imtstr] > bias_max_dsigma:
                bias[imtstr] = 0.0
                sigma_2_eta[imtstr] = 0.0
        else:
            bias[imtstr] = 0.0
            sigma_2_eta[imtstr] = 0.0
        bias_time = time.time() - time1
        if verbose:
            print('%s: bias %f stddev %f (time=%f sec)' % 
                  (imtstr, bias[imtstr], np.sqrt(sigma_2_eta[imtstr]), 
                   bias_time))





#    imt_out_set_str = set(['PGV'])

    for imtstr in imt_out_set_str:
        time1 = time.time()
        #
        # Get the (pesudo-) period of the output IMT
        #
        oqimt = imt.from_string(imtstr)
        if imtstr == 'PGA':
            outperiod_ix = imt_per_ix['0.01']
        elif imtstr in ('PGV', 'MMI'):
            outperiod_ix = imt_per_ix['1.0']
        else:
            outperiod_ix = imt_per_ix[str(oqimt.period)]
        #
        # Get the predictions at the output points
        #
#
# Should probably check to see if there is a bias and if not, select the
# total uncertainty rather than the intra even uncertainty...
#
        pout_mean, pout_sd = gmas(ipe, gmpe, sx_out_soil, rx, dx_out, oqimt, 
                                  stddev_types)
        pout_sd = pout_sd[2]
        pout_sd2 = np.power(pout_sd, 2.0)

        #
        # Bias the predictions
        #
        pout_mean += bias[imtstr]
        #
        # Remake the residual array now that we (may) have a bias
        #
        if bias[imtstr] != 0:
            sta_resids[imtstr] = np.array([])        
            for ndf, sdf in df_dict.items():
                for imtin in get_imts(imtstr, imt_in_str_dict[ndf]):
                    vidx = ~np.isnan(sdf[imtin])
                    sta_resids[imtstr] = np.append(
                            sta_resids[imtstr], 
                            sdf[imtin + '_residual'][vidx] - bias[imtstr])
        sta_resids[imtstr] = sta_resids[imtstr].reshape((-1, 1))
        #
        # Now do the MVN itself...
        #
        dtime = 0
        mtime = 0
        ddtime = 0
        ctime = stime = atime = 0

        ampgrid = np.zeros_like(pout_mean)
        sdgrid = np.zeros_like(pout_mean)
        corr_adj12 = corr_adj[imtstr] * np.ones((1, smnx))
        for iy in range(smny):
            ss = iy * smnx
            se = (iy + 1) * smnx
            time4 = time.time()
            dist12 = geodetic_distance(lons_out_rad[ss:se].reshape(1, -1), 
                                       lats_out_rad[ss:se].reshape(1, -1),
                                       sta_lons_rad[imtstr], sta_lats_rad[imtstr])
            t2_12 = np.full(dist12.shape, outperiod_ix, dtype=np.int)
            d12_rows, d12_cols = np.shape(dist12)
            t1_12 = np.tile(sta_period_ix[imtstr], (1, d12_cols))
            ddtime += time.time() - time4
            time4 = time.time()
            corr12 = ccf.getCorrelation(t1_12, t2_12, dist12)
            ctime += time.time() - time4
            time4 = time.time()
            psd = pout_sd[iy, :].reshape((1, -1))
            ss = sta_sigs[imtstr]
            sigma12 =  ne.evaluate(
                    "corr12 * corr_adj12 * (ss * psd)"
                    ).T
            stime += time.time() - time4    
            time4 = time.time()
            rcmatrix = sigma12.dot(sigma22inv[imtstr])
            dtime += time.time() - time4    
            time4 = time.time()
            ampgrid[iy, :] = pout_mean[iy, :] + ((corr_adj12.T * 
                   rcmatrix).dot(sta_resids[imtstr])).reshape((-1,))
            atime += time.time() - time4
            time4 = time.time()
    #        sdgrid[ss:se] = pout_sd2[ss:se] - np.diag(rcmatrix.dot(sigma12))
            sdgrid[iy, :] = pout_sd2[iy, :] - \
                    np.sum(rcmatrix * sigma12, axis=1)
            mtime += time.time() - time4
                
        outgrid[imtstr] = ampgrid
        sdgrid[sdgrid < 0] = 0
        outsd[imtstr] = np.sqrt(sdgrid)
        if verbose:
            print('time for %s distance=%f' % (imtstr, ddtime))
            print('time for %s correlation=%f' % (imtstr, ctime))
            print('time for %s sigma=%f' % (imtstr, stime))
            print('time for %s rcmatrix=%f' % (imtstr, dtime))
            print('time for %s amp calc=%f' % (imtstr, atime))
            print('time for %s sd calc=%f' % (imtstr, mtime))
            print('total time for %s=%f' % (imtstr, time.time() - time1))

#%%
    #------------------------------------------------------------------
    # Output the data and metadata
    #------------------------------------------------------------------
    oc = OutputContainer.createEmpty(os.path.join(datadir, 'shake_result.hdf'))
    #
    # Might as well stick the whole config in the result
    #
    oc.addMetadata(config, name='config')
    #
    # Clean up the metadata, and add new stuff
    #
    metadata = copy.deepcopy(config)
    del metadata['ccf_modules']
    del metadata['gmice_modules']
    del metadata['gmpe_modules']
    del metadata['gmpe_sets']
    del metadata['ipe_modules']
    del metadata['grind']['vs30file']
    metadata['grind']['ccf'] = config['ccf_modules'][config['grind']['ccf']][0]
    metadata['grind']['ipe'] = config['ipe_modules'][config['grind']['ipe']][0]
    metadata['grind']['gmice'] = \
            config['gmice_modules'][config['grind']['gmice']][0]
    metadata['bias'] = {}
    for key, value in bias.items():
        metadata['bias'][key] = value
    metadata['max_amp'] = {}
    for key, value in outgrid.items():
        metadata['max_amp'][key] = np.max(outgrid[key])
        #
        # How do we get max value on land?
        #
    #
    # Need:
    #   event info
    #   rupture info (how?)
    #   number of stations, dyfi
    #   ShakeMap grade (how?)
    #   ShakeMap version, map version, date of production
    #   station data
    #
    oc.addMetadata(metadata)
    
    for key, value in outgrid.items():
        oc.addData(value, key)
        oc.addData(outsd[key], key + '_sd')
    
    oc.close()
    print('done')
    #------------------------------------------------------------------
    # End grind()
    #------------------------------------------------------------------
    
#%%
if __name__ == '__main__':
    description = '''Process a shakemap...
The only argument is a ShakeMap event ID, which should correspond to a 
directory in the ShakeMap data directory of the current profile.
'''
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('eventid', 
                    help='Path to ShakeMap data directory containing '
                         'input and config files.')
    parser.add_argument('-v','--verbose', action='store_true',
                        help='Print informational messages.')
    pargs = parser.parse_args()
    grind(pargs)


