#!/usr/bin/env python

import sys
import os.path
import time as time
import argparse
from importlib import import_module
from time import gmtime, strftime

# Not really needed, but handy for looking at the config
import pprint

import numpy as np
import numexpr as ne

from openquake.hazardlib import imt
import openquake.hazardlib.const as oqconst
import openquake.hazardlib.gsim.base as oqbase

from shakelib.grind.rupture import PointRupture
from shakelib.grind.sites import Sites
from shakelib.grind.distance import Distance, get_distance
from shakelib.grind.multigmpe import MultiGMPE
from shakelib.grind.virtualipe import VirtualIPE

from shakelib.grind.container import InputContainer, OutputContainer
from shakemap.utils.config import get_config_paths

#%%
#
# helper function for things (ipe, gmice, ccf) that don't have a 
# fromConfig() constructor yet. Instantiates an instance of a 
# class from config entry, 'name', that has a corresponding 
# 'name'_module dictionary of class name, module path
#
def get_object_from_config(obj, cfg, *args):
    cls_abbr = cfg['grind'][obj]
    mods = obj + '_modules'
    (cname, mpath) = cfg[mods][cls_abbr]
    return getattr(import_module(mpath), cname)(*args)

#
# Set the attributes of the site context
#        
def set_sx_params(sx, lons, lats, vs30):      
    sx.lons            = lons
    sx.lats            = lats
    sx.vs30            = vs30
    sx.z1pt0_cy14_cal  = Sites._z1pt0_from_vs30_cy14_cal(sx.vs30)
    sx.z1pt0_ask14_cal = Sites._z1pt0_from_vs30_ask14_cal(sx.vs30)
    sx.z2pt5_cb14_cal  = Sites._z2pt5_from_vs30_cb14_cal(sx.vs30) / 1000.0
    sx.z1pt0_cy08      = Sites._z1pt0_from_vs30_cy08(sx.vs30)
    sx.z2pt5_cb07      = Sites._z2pt5_from_z1pt0_cb07(sx.z1pt0_cy08)
    sx.vs30measured    = np.zeros_like(lons)
    sx.backarc         = np.zeros_like(lons)
    return sx

#
# Helper function to call get_mean_and_stddevs for the 
# appropriate object given the IMT
#
def gmas(ipe, gmpe, sx, rx, dx, oqimt, stddev_types):
    if 'MMI' in oqimt:
        pe = ipe
    else:
        pe = gmpe
    return pe.get_mean_and_stddevs(sx, rx, dx, oqimt, stddev_types)

#: Earth radius in km.
EARTH_RADIUS = 6371.0
#
# Quick and dirty distance calculator; uses numexpr for speed
#
def geodetic_distance(lons1, lats1, lons2, lats2, diameter=2*EARTH_RADIUS):
    d = ne.evaluate("EARTH_RADIUS * sqrt(((lons1 - lons2) * cos(0.5 * " \
                    "(lats1 + lats2)))**2.0 + (lats1 - lats2)**2.0)")
    return d

def get_period_index_from_imt_str(imtstr, imt_per_ix):
        if imtstr == 'PGA':
            return imt_per_ix['0.01']
        elif imtstr in ('PGV', 'MMI'):
            return imt_per_ix['1.0']
        else:
            return imt_per_ix[imtstr.replace('SA(', '').replace(')', '')]

def get_period_array(*args):
    imt_per = set()
    for imt_list in args:
        for imtstr in imt_list:
            if imtstr == 'PGA':
                imt_per.add(0.01)
            elif imtstr == 'PGV' or imtstr == 'MMI':
                imt_per.add(1.0)
            else:
                imt_per.add(float(imtstr.replace('SA(', '').replace(')', '')))
    return np.array(sorted(imt_per))

def get_imts(imtstr, imtset):

    if imtstr in imtset:
        return (imtstr, )

    salist = [x for x in imtset if x.startswith('SA(')]
    periodlist = [float(x.replace('SA(', '').replace(')', '')) for x in salist]
    periodlist = sorted(periodlist)
    periodlist_str = [str(x) for x in periodlist]
    
    #
    # If we're here, then we know that IMT isn't in the inputs. Try
    # some alternatives.
    #
    if imtstr == 'PGA':
        #
        # Use the highest frequency in the inputs, otherwise use PGV
        #
        if len(salist):
            return ('SA(' + periodlist_str[0] + ')', )
        elif 'PGV' in imtset:
            return ('PGV', )
        else:
            return ()
    elif imtstr == 'PGV':
        #
        # Use 1.0 sec SA (or its bracket) if it's there, otherwise
        # use PGA
        #
        sa_tuple = get_sa_bracket(1.0, periodlist, periodlist_str)
        if sa_tuple != ():
            return sa_tuple
        if 'PGA' in imtset:
            return ('PGA', )
        else:
            return ()            
    elif imtstr == 'MMI':
        #
        # Use PGV if it's there, otherwise use 1.0 sec SA (or its
        # bracket)
        #
        if 'PGV' in imtset:
            return ('PGV', )
        return get_sa_bracket(1.0, periodlist, periodlist_str)
    elif imtstr.startswith('SA('):
        myper = float(imtstr.replace('SA(', '').replace(')', ''))
        return get_sa_bracket(myper, periodlist, periodlist_str)
    else:
        raise ValueError('Unknown IMT %s in get_imt_bracket' % imtstr)
        
def get_sa_bracket(myper, plist, plist_str):
        
    if not len(plist):
        return ()
    try:
        return ('SA(' + plist_str[plist.index(myper)] + ')', )
    except ValueError:
        pass
    for i, v in enumerate(plist):
        if v > myper:
            break
    if i == 0 or v < myper:
        return ('SA(' + plist_str[i] + ')', )
    else:
        return ('SA(' + plist_str[i-1] + ')', 'SA(' + plist_str[i] + ')')

def get_sta_imts(imtstr, sdf, ix, imtset):
    myimts = set()
    for this_imt in imtset:
        if not np.isnan(sdf[this_imt][ix]) and \
           not sdf[this_imt + '_outliers'][ix]:
            myimts.add(this_imt)
    return get_imts(imtstr, myimts)
#%%
args = type('Dummy', (object,), {'eventid' : 'northridge',
                                 'verbose' : True})
if True:
#def grind(args):
    verbose = args.verbose
    #
    # Find the shake_data file
    #
    install_path, data_path = get_config_paths()
    datadir = os.path.join(data_path, args.eventid)
    if not os.path.isdir(datadir):
        print('%s is not a valid directory.' % datadir)
        sys.exit(1)
    datafile = os.path.join(datadir, 'shake_data.hdf')
    if not os.path.isfile(datafile):
        print('%s is not a valid shake data file.' % datafile)
        sys.exit(1)
    #------------------------------------------------------------------
    # Make the input container and extract the config
    #------------------------------------------------------------------
    ic = InputContainer.loadFromHDF(datafile)
    config = ic.getConfig()
#    pprint.pprint(config)
    #------------------------------------------------------------------
    # Instantiate the gmpe, gmice, ipe, and ccf
    #------------------------------------------------------------------
    gmpe = MultiGMPE.from_config(config)

    gmice = get_object_from_config('gmice', config)

    if config['ipe_modules'][config['grind']['ipe']][0] == 'VirtualIPE':
        ipe = VirtualIPE.fromFuncs(gmpe, gmice)
    else:
        ipe = get_object_from_config('ipe', config)
    #------------------------------------------------------------------
    # Bias parameters
    #------------------------------------------------------------------
    do_bias         = config['grind']['bias']['do_bias']
    bias_max_range  = config['grind']['bias']['max_range']
    bias_max_mag    = config['grind']['bias']['max_mag']
    bias_max_dsigma = config['grind']['bias']['max_delta_sigma']
    #------------------------------------------------------------------
    # Outlier parameters
    #------------------------------------------------------------------
    outlier_deviation_level = config['grind']['outlier']['max_deviation']
    outlier_max_mag = config['grind']['outlier']['max_mag']
    #------------------------------------------------------------------
    # These are the IMTs we want to make
    #------------------------------------------------------------------
    imt_out_set_str = set(config['grind']['imt_list'])
    imt_out_set_str.add('SA(2.0)')
    imt_out_set = [imt.from_string(x) for x in imt_out_set_str]
    #------------------------------------------------------------------
    # Get the rupture object and rupture context
    #------------------------------------------------------------------
    rupture_obj = ic.getRupture()
    rx = rupture_obj.getRuptureContext([gmpe])
    if rx.rake == None:
        rx.rake = 0
    #------------------------------------------------------------------
    # Get the Vs30 file name
    #------------------------------------------------------------------
    vs30default = config['grind']['vs30default']
    vs30_file = config['grind']['vs30file']
    if not vs30_file:
        vs30_file = None 
    #------------------------------------------------------------------
    # The output locations: either a grid or a list of points
    #------------------------------------------------------------------
    if config['grind']['prediction_location']['file']:
        #
        # FILE: Open the file and get the output points
        #
        lons, lats, idents = np.genfromtxt(
                        config['grind']['prediction_location']['file'],
                        autostrip=True, unpack=True)
        depths = np.zeros_like(lats)
        smnx = np.size(lons)
        smny = 1
        dist_obj_out = Distance(gmpe, lons, lats, depths, rupture_obj)
        #
        # Get the Vs30 from a file (could add other params here)
        # In the future we may want to support selecting
        # the Vs30 values from a grid
        #
        vs30_rock = np.full_like(lons, vs30default)
        if vs30_file:
            ids, vs = np.genfromtxt(vs30_file, autostrip=True, unpack=True)
            vs30_hash = dict(zip(ids, vs))
            vs30 = np.ndarray([vs30_hash[x] for x in idents])
        else:
            vs30 = vs30_rock
        sx_out_soil = oqbase.SitesContext()
        sx_out_rock = oqbase.SitesContext()
        
        sx_out_soil = set_sx_params(sx_out_soil, lons, lats, vs30)
        sx_out_rock = set_sx_params(sx_out_rock, lons, lats, vs30_rock)
    else:
        #
        # GRID: Figure out the grid parameters and get output points
        #
        smdx = config['grind']['prediction_location']['xres']
        smdy = config['grind']['prediction_location']['yres']

        W, S, E, N = config['grind']['prediction_location']['extent']
        
        sites_obj_out = Sites.fromBounds(W, E, S, N, smdx, smdy, 
                                         defaultVs30=vs30default, 
                                         vs30File=vs30_file)
        smnx, smny = sites_obj_out.getNxNy()
        
        sx_out_soil = sites_obj_out.getSitesContext()
        sx_out_rock = sites_obj_out.getSitesContext(rock_vs30=vs30default)
        lons, lats = np.meshgrid(sx_out_rock.lons, sx_out_rock.lats)
        lons = np.flipud(lons)
        lats = np.flipud(lats)
        lons = lons.flatten()
        lats = lats.flatten()
        depths = np.zeros_like(lats)

        dist_obj_out = Distance.fromSites(gmpe, sites_obj_out, rupture_obj)
    
    dx_out = dist_obj_out.getDistanceContext()
        
    lons_out_rad = np.radians(lons)
    lats_out_rad = np.radians(lats)
    #------------------------------------------------------------------
    # Station data
    #------------------------------------------------------------------
    stations = ic.getStationList()
    stddev_types = [oqconst.StdDev.TOTAL, oqconst.StdDev.INTER_EVENT, 
                    oqconst.StdDev.INTRA_EVENT]
    #
    # df1 holds the instrumented data (PGA, PGV, SA)
    # df2 holds the non-instrumented data (MMI)
    #
    df_dict = {'df1': None, 'df2': None}
    imt_in_str_dict = {'df1': None, 'df2': None}
    imt_in_dict = {'df1': None, 'df2': None}
    imt_in_str_set = set()
    sx_dict = {'df1': None, 'df2': None}
    dx_dict = {'df1': None, 'df2': None}
    if stations is not None:
        df_dict['df1'] = stations.getStationDataframe(1)
        df_dict['df2'] = stations.getStationDataframe(0)
        #
        # Get the sites and distance contexts for each dataframe then
        # compute the predictions for the IMTs in that dataframe.
        #
        for ndf, df in df_dict.items():
            if not df:
                continue
            #
            # Get lists of the input IMTs
            #
            imt_in_str_dict[ndf] = set(
                    [x for x in df.keys() if x in ('PGA', 'PGV', 'MMI') or 
                     x.startswith('SA(')])
            imt_in_dict[ndf] = set(
                    [imt.from_string(x) for x in imt_in_str_dict[ndf]])
            imt_in_str_set |= imt_in_str_dict[ndf]
            #
            # Get the sites and distance contexts
            #
            df['depth'] = np.zeros_like(df['lon'])
            lldict = {'lons': df['lon'], 'lats': df['lat']}
            sx_dict[ndf] = sites_obj_out.getSitesContext(lldict)
            dist_obj = Distance(gmpe, df['lon'], df['lat'], df['depth'], 
                                rupture_obj)
            dx_dict[ndf] = dist_obj.getDistanceContext()
            #
            # Do the predictions and other bookkeeping for each IMT
            #
            for imtstr in imt_in_str_dict[ndf]:
                pmean, psd = gmas(ipe, gmpe, sx_dict[ndf], rx, dx_dict[ndf], 
                                  imt.from_string(imtstr), stddev_types)
                df[imtstr + '_pred'] = pmean
                df[imtstr + '_pred_sd_total'] = psd[0]
                df[imtstr + '_pred_sd_inter'] = psd[1]
                df[imtstr + '_pred_sd_intra'] = psd[2]
                #
                # Compute the total residual
                #
                df[imtstr + '_residual'] = df[imtstr] - df[imtstr + '_pred']
                #
                # Do the outlier flagging if we don't have a fault and
                # the event magnitude is over the limit
                #
                if not isinstance(rupture_obj, PointRupture) or \
                   rx.mag <= outlier_max_mag:
                    #
                    # turn off nan warnings for this statement
                    #
                    np.seterr(invalid='ignore')
                    flagged = np.abs(df[imtstr + '_residual']) > \
                              outlier_deviation_level * \
                              df[imtstr + '_pred_sd_total']
                    np.seterr(invalid='warn')
                    if verbose:
                        print('IMT: %s, flagged: %d' % 
                              (imtstr, np.sum(flagged)))
                    df[imtstr + '_outliers'] = flagged
                else:
                    df[imtstr + '_outliers'] = np.full(df[imtstr].shape, True, 
                                                       dtype=np.bool)
                #
                # Make the uncertainty arrays for any IMTs that don't
                # have them.
                #
                if (imtstr + '_sd') not in df:
                    if imtstr == 'MMI':
                        sdval = 0.3
                    else:
                        sdval = 0.0
                    df[imtstr + '_sd'] = np.full_like(df['lon'], sdval)
            #
            # Get the lons/lats in radians while we're at it
            #
            df['lon_rad'] = np.radians(df['lon'])
            df['lat_rad'] = np.radians(df['lat'])
            #
            # It will be handy later on to have the rupture distance
            # in the dataframes
            #
            dd = get_distance(['rrup'], df['lat'], df['lon'], 
                                      df['depth'], rupture_obj)
            df['rrup'] = dd['rrup']
    df1 = df_dict['df1']
    df2 = df_dict['df2']

#%%
    #------------------------------------------------------------------
    # Compute all the IMTs possible from MMI
    # This logic needs to be revisited. We should probably make what
    # we have to to do the CMS to make the needed output IMTs, but
    # for now, we're just going to use what we have and the ccf.
    #------------------------------------------------------------------
    if df2:
        for gmice_imt in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            if imt.SA == gmice_imt:
                iterlist = gmice.DEFINED_FOR_SA_PERIODS                    
            else:
                iterlist = [None]
            for period in iterlist:
                oqimt = gmice_imt(period)
                imtstr = str(oqimt)
                df2[imtstr], _ = gmice.getGMfromMI(df2['MMI'], oqimt, 
                                                   dists=df2['rrup'], 
                                                   mag=rx.mag)
                np.seterr(invalid='ignore')
                df2[imtstr][df2['MMI'] < 4.0] = np.nan
                np.seterr(invalid='warn')
                df2[imtstr + '_sd'] = np.full_like(df2['MMI'],
                       gmice.getMI2GMsd()[oqimt])
                imt_in_str_dict['df2'].add(imtstr)
                #
                # Get the predictions and stddevs, too
                #
                pmean, psd = gmas(ipe, gmpe, sx_dict['df2'], rx, 
                                  dx_dict['df2'], oqimt, stddev_types)
                df2[imtstr + '_pred'] = pmean
                df2[imtstr + '_pred_sd_total'] = psd[0]
                df2[imtstr + '_pred_sd_inter'] = psd[1]
                df2[imtstr + '_pred_sd_intra'] = psd[2]
                df2[imtstr + '_residual'] = df2[str(oqimt)] - pmean
                df2[imtstr + '_outliers'] = np.full(pmean.shape, False, 
                                                    dtype=np.bool)
                
    #
    # Now make derived MMI from the best available PGM; This is ugly and it
    # would be nice to have a more deterministic way of doing it
    #
    if df1:
        imtstr = None
        if 'PGV' in df1 \
                and imt.PGV in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            imtstr = 'PGV'
        elif 'PGA' in df1 \
                and imt.PGA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            imtstr = 'PGA'
        elif 'SA(1.0)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 1.0 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(1.0)'
        elif 'SA(0.3)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 0.3 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(0.3)'
        elif 'SA(3.0)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 3.0 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(3.0)'
        
        if imtstr is not None:
            oqimt = imt.from_string(imtstr)
            df1['MMI'], _ = gmice.getMIfromGM(df1[imtstr], oqimt,
                                              dists=df1['rrup'], mag=rx.mag)
            df1['MMI_sd'] = np.full_like(df1[imtstr], 
                                         gmice.getGM2MIsd()[oqimt])
            imt_in_str_dict['df1'].add('MMI')
            #
            # Get the prediction and stddevs
            #
            pmean, psd = gmas(ipe, gmpe, sx_dict['df1'], rx, dx_dict['df1'], 
                              imt.from_string('MMI'), stddev_types)
            df1['MMI' + '_pred'] = pmean
            df1['MMI' + '_pred_sd_total'] = psd[0]
            df1['MMI' + '_pred_sd_inter'] = psd[1]
            df1['MMI' + '_pred_sd_intra'] = psd[2]
            df1['MMI' + '_residual'] = df1['MMI'] - pmean
            df1['MMI' + '_outliers'] = np.full(pmean.shape, False, 
                                               dtype=np.bool)

    imt_per = get_period_array(imt_in_str_dict['df1'], 
                               imt_in_str_dict['df2'],
                               imt_out_set_str)
    ccf = get_object_from_config('ccf', config, imt_per)

    imt_per_ix = {str(per) : ix for ix, per in enumerate(imt_per)}
#%%
    #------------------------------------------------------------------
    # Do the MVN
    #------------------------------------------------------------------
    #
    # First do the bias for all of the input and output IMTs. Hold on
    # to some of the products that will be used for the interpolation.
    #
    bias = {}
    sigma_2_eta = {}
    outgrid = {}
    outsd = {}
    
    sta_lons_rad = {}
    sta_lats_rad = {}
    sta_resids = {}
    sta_period_ix = {}
    sta_sigs = {}
    sta_sigs_total = {}
    sta_sigs_inter = {}
    corr_adj = {}
    sigma22inv = {}
    
    #
    # Python scoping is weird. But ths works. Seemingly.
    # The only reason to have this function is to avoid repeating
    # all this code, and the reason to make it nested is to avoid
    # having to pass eleven arguments.
    #
    def build_list(imtstr, imtin, sdf, vidx):
        inperiod_ix = get_period_index_from_imt_str(imtin, imt_per_ix)
        sta_lons_rad[imtstr] = np.append(sta_lons_rad[imtstr], 
                                         sdf['lon_rad'][vidx])
        sta_lats_rad[imtstr] = np.append(sta_lats_rad[imtstr], 
                                         sdf['lat_rad'][vidx])
        sta_resids[imtstr] = np.append(sta_resids[imtstr], 
                               sdf[imtin + '_residual'][vidx])
        psd = sdf[imtin + '_pred_sd_intra'][vidx]
        sta_sigs[imtstr] = np.append(sta_sigs[imtstr], psd)
        sta_sigs_inter[imtstr] = np.append(sta_sigs_inter[imtstr], 
                      sdf[imtin + '_pred_sd_inter'][vidx])
        sta_sigs_total[imtstr] = np.append(sta_sigs_total[imtstr], 
                      np.sqrt(psd**2 + sdf[imtin + '_sd'][vidx]**2))
        if isinstance(vidx, int):
            per_val = inperiod_ix
        else:
            per_val = np.full(psd.shape, inperiod_ix, dtype=np.int64)
        sta_period_ix[imtstr] = np.append(sta_period_ix[imtstr], per_val)
        return

    combined_imt_str = imt_in_str_dict['df1'] | imt_in_str_dict['df2'] \
                                              | imt_out_set_str
    for imtstr in combined_imt_str:
        time1 = time.time()
        #
        # Get the index of the (pesudo-) period of the output IMT
        #
        outperiod_ix = get_period_index_from_imt_str(imtstr, imt_per_ix)
        #
        # Fill the station arrays
        #        
        sta_lons_rad[imtstr] = np.array([])
        sta_lats_rad[imtstr] = np.array([])
        sta_resids[imtstr] = np.array([])
        sta_sigs[imtstr] = np.array([])
        sta_sigs_total[imtstr] = np.array([])
        sta_sigs_inter[imtstr] = np.array([])
        sta_period_ix[imtstr] = np.array([], dtype=np.int64)
        for ndf, sdf in df_dict.items():
            if ndf == 'df1':
                for i in range(np.size(sdf['lon'])):
                    for imtin in get_sta_imts(imtstr, sdf, i, 
                                              imt_in_str_dict[ndf]):
                        build_list(imtstr, imtin, sdf, i)
            else:                       
                for imtin in get_imts(imtstr, imt_in_str_dict['df2']):
                    vidx = ~(np.isnan(sdf[imtin]) | sdf[imtin + '_outliers'])
                    build_list(imtstr, imtin, sdf, vidx)       
        if np.size(sta_lons_rad[imtstr]) == 0:
            bias[imtstr] = 0.0
            sigma_2_eta[imtstr] = 0.0
            continue
        sta_lons_rad[imtstr] = sta_lons_rad[imtstr].reshape((-1, 1))
        sta_lats_rad[imtstr] = sta_lats_rad[imtstr].reshape((-1, 1))
        sta_resids[imtstr] = sta_resids[imtstr].reshape((-1, 1))
        sta_sigs[imtstr] = sta_sigs[imtstr].reshape((-1, 1))
        sta_sigs_total[imtstr] = sta_sigs_total[imtstr].reshape((-1, 1))
        sta_period_ix[imtstr] = sta_period_ix[imtstr].reshape((-1, 1))
        corr_adj[imtstr] = (sta_sigs[imtstr] / sta_sigs_total[imtstr])
        corr_adj22 = corr_adj[imtstr] * corr_adj[imtstr].T
        np.fill_diagonal(corr_adj22, 1.0)
        dist22 = geodetic_distance(sta_lons_rad[imtstr], 
                                   sta_lats_rad[imtstr],
                                   sta_lons_rad[imtstr].T, 
                                   sta_lats_rad[imtstr].T)
        d22_rows, d22_cols = np.shape(dist22) # should be square
        t1_22 = np.tile(sta_period_ix[imtstr], (1, d22_cols))
        t2_22 = np.tile(sta_period_ix[imtstr].T, (d22_rows, 1))
        corr22 = ccf.getCorrelation(t1_22, t2_22, dist22) * corr_adj22
        sigma22 = corr22 * (sta_sigs[imtstr] * sta_sigs[imtstr].T)
        sigma22inv[imtstr] = np.linalg.pinv(sigma22)
        #
        # Compute the bias and apply it to the predictions and the
        # residuals
        #
        if do_bias and (not isinstance(rupture_obj, PointRupture) 
                        or rx.mag <= bias_max_mag):        
            ONE = np.ones_like(sta_resids[imtstr])
            tau = np.mean(sta_sigs_inter[imtstr])
            sigma_2_eta[imtstr] = 1.0 / (1 / tau**2 + \
                                   ONE.T.dot(sigma22inv[imtstr].dot(ONE)))
            bias[imtstr] = \
                    ONE.T.dot(sigma22inv[imtstr].dot(sta_resids[imtstr])) * \
                    sigma_2_eta[imtstr]
            if bias[imtstr] > bias_max_dsigma:
                bias[imtstr] = 0.0
                sigma_2_eta[imtstr] = 0.0
        else:
            bias[imtstr] = 0.0
            sigma_2_eta[imtstr] = 0.0
        bias_time = time.time() - time1
        if verbose:
            print('%s: bias %f stddev %f; %d stations (time=%f sec)' % 
                  (imtstr, bias[imtstr], np.sqrt(sigma_2_eta[imtstr]), 
                   np.size(sta_lons_rad[imtstr]), bias_time))
    #
    # End bias
    #
    #
    # Now do the MVN with the intra-event residuals
    #
    for imtstr in imt_out_set_str:
        time1 = time.time()
        #
        # Get the index of the (pesudo-) period of the output IMT
        #
        outperiod_ix = get_period_index_from_imt_str(imtstr, imt_per_ix)
        #
        # Get the predictions at the output points
        #
#
# Should probably check to see if there is a bias and if not, select the
# total uncertainty rather than the intra even uncertainty...
#
        oqimt = imt.from_string(imtstr)
        pout_mean, pout_sd = gmas(ipe, gmpe, sx_out_soil, rx, dx_out, oqimt, 
                                  stddev_types)
        pout_sd = pout_sd[2]
        pout_sd2 = np.power(pout_sd, 2.0)

        #
        # Bias the predictions
        #
        pout_mean += bias[imtstr]
        #
        # If there are no data, just use the unbiased prediction
        #
        if np.size(sta_lons_rad[imtstr]) == 0:
            outgrid[imtstr] = pout_mean
            outsd[imtstr] = pout_sd[0]
            continue
        #
        # Remake the residual array now that we (may) have a bias
        #
        if bias[imtstr] != 0:            
            sta_resids[imtstr] = np.array([])        
            sdf = df_dict['df1']
            for i in range(np.size(sdf['lon'])):
                for imtin in get_sta_imts(imtstr, sdf, i, imt_in_str_dict['df1']):
                    sta_resids[imtstr] = np.append(
                            sta_resids[imtstr], 
                            sdf[imtin + '_residual'][i] - bias[imtin])        
            sdf = df_dict['df2']        
            for imtin in get_imts(imtstr, imt_in_str_dict['df2']):
                vidx = ~(np.isnan(sdf[imtin]) | sdf[imtin + '_outliers'])
                sta_resids[imtstr] = np.append(
                        sta_resids[imtstr], 
                        sdf[imtin + '_residual'][vidx] - bias[imtin])
        sta_resids[imtstr] = sta_resids[imtstr].reshape((-1, 1))
        #
        # Now do the MVN itself...
        #
        dtime = 0
        mtime = 0
        ddtime = 0
        ctime = stime = atime = 0

        ampgrid = np.zeros_like(pout_mean)
        sdgrid = np.zeros_like(pout_mean)
        corr_adj12 = corr_adj[imtstr] * np.ones((1, smnx))
        for iy in range(smny):
            ss = iy * smnx
            se = (iy + 1) * smnx
            time4 = time.time()
            dist12 = geodetic_distance(lons_out_rad[ss:se].reshape(1, -1), 
                                       lats_out_rad[ss:se].reshape(1, -1),
                                       sta_lons_rad[imtstr], sta_lats_rad[imtstr])
            t2_12 = np.full(dist12.shape, outperiod_ix, dtype=np.int)
            d12_rows, d12_cols = np.shape(dist12)
            t1_12 = np.tile(sta_period_ix[imtstr], (1, d12_cols))
            ddtime += time.time() - time4
            time4 = time.time()
            corr12 = ccf.getCorrelation(t1_12, t2_12, dist12)
            ctime += time.time() - time4
            time4 = time.time()
            psd = pout_sd[iy, :].reshape((1, -1))
            ss = sta_sigs[imtstr]
            sigma12 =  ne.evaluate(
                    "corr12 * corr_adj12 * (ss * psd)"
                    ).T
            stime += time.time() - time4    
            time4 = time.time()
            rcmatrix = sigma12.dot(sigma22inv[imtstr])
            dtime += time.time() - time4    
            time4 = time.time()
            ampgrid[iy, :] = pout_mean[iy, :] + ((corr_adj12.T * 
                   rcmatrix).dot(sta_resids[imtstr])).reshape((-1,))
            atime += time.time() - time4
            time4 = time.time()
    #        sdgrid[ss:se] = pout_sd2[ss:se] - np.diag(rcmatrix.dot(sigma12))
            sdgrid[iy, :] = pout_sd2[iy, :] - \
                    np.sum(rcmatrix * sigma12, axis=1)
            mtime += time.time() - time4
                
        outgrid[imtstr] = ampgrid
        sdgrid[sdgrid < 0] = 0
        outsd[imtstr] = np.sqrt(sdgrid)
        if verbose:
            print('\ttime for %s distance=%f' % (imtstr, ddtime))
            print('\ttime for %s correlation=%f' % (imtstr, ctime))
            print('\ttime for %s sigma=%f' % (imtstr, stime))
            print('\ttime for %s rcmatrix=%f' % (imtstr, dtime))
            print('\ttime for %s amp calc=%f' % (imtstr, atime))
            print('\ttime for %s sd calc=%f' % (imtstr, mtime))
            print('total time for %s=%f' % (imtstr, time.time() - time1))

#%%
    #------------------------------------------------------------------
    # Output the data and metadata
    #------------------------------------------------------------------
    oc = OutputContainer.createEmpty(os.path.join(datadir, 'shake_result.hdf'))
    #
    # Might as well stick the whole config in the result
    #
    oc.addMetadata(config, name='config')
    
    metadata = {}
    metadata['input'] = {}
    metadata['input']['event_information'] = {}
    metadata['input']['event_information']['depth'] = rx.hypo_depth
    metadata['input']['event_information']['event_id'] = args.eventid
    metadata['input']['event_information']['fault_ref'] = rupture_obj._reference
    metadata['input']['event_information']['faultfiles'] = ''                                       ################
    metadata['input']['event_information']['feregion'] = config['zone_info']['fename']
    metadata['input']['event_information']['intensity_observations'] = np.size(df2['lon'])
    metadata['input']['event_information']['latitude'] = rx.hypo_lat
    metadata['input']['event_information']['longitude'] = rx.hypo_lon
    metadata['input']['event_information']['location'] = rupture_obj._origin.locstring
    metadata['input']['event_information']['magnitude'] = rx.mag
    metadata['input']['event_information']['mech_src'] = config['zone_info']['moment_tensor_source']
    metadata['input']['event_information']['origin_time'] = rupture_obj._origin.time.strftime('%Y-%m-%d %H:%M:%S')
    metadata['input']['event_information']['seismic_stations'] = np.size(df1['lon'])
    metadata['input']['event_information']['src_mech'] = rupture_obj._origin.mech
    metadata['input']['event_information']['tectonic_regime'] = config['zone_info']['domain']
    metadata['input']['event_information']['event_description'] = rupture_obj._origin.locstring     ################
    metadata['input']['event_information']['event_type'] = rupture_obj._origin.mech                 ################
    metadata['output'] = {}
    metadata['output']['ground_motions'] = {}
    metadata['output']['ground_motions']['intensity'] = {}
    metadata['output']['ground_motions']['intensity']['units'] = 'MMI'
    metadata['output']['ground_motions']['intensity']['bias'] = bias['MMI']
    metadata['output']['ground_motions']['intensity']['max_grid'] = np.max(outgrid['MMI'])
    metadata['output']['ground_motions']['intensity']['max'] = np.max(outgrid['MMI'])         ################
    metadata['output']['ground_motions']['pga'] = {}
    metadata['output']['ground_motions']['pga']['units'] = 'g'
    metadata['output']['ground_motions']['pga']['bias'] = bias['PGA']
    metadata['output']['ground_motions']['pga']['max_grid'] = np.max(outgrid['PGA'])
    metadata['output']['ground_motions']['pga']['max'] = np.max(outgrid['PGA'])         ################
    metadata['output']['ground_motions']['pgv'] = {}
    metadata['output']['ground_motions']['pgv']['units'] = 'cms'
    metadata['output']['ground_motions']['pgv']['bias'] = bias['PGV']
    metadata['output']['ground_motions']['pgv']['max_grid'] = np.max(outgrid['PGV'])
    metadata['output']['ground_motions']['pgv']['max'] = np.max(outgrid['PGV'])         ################
    #
    # Meet with HazDev to revise these old-style SA tags
    #
    for imtstr in imt_out_set_str:
        if not imtstr.startswith('SA('):
            continue
        imtper = imtstr.replace('SA(', '').replace('.', '').replace(')', '')
        newimt = 'psa' + imtper
        metadata['output']['ground_motions'][newimt] = {}
        metadata['output']['ground_motions'][newimt]['units'] = 'g'
        metadata['output']['ground_motions'][newimt]['bias'] = bias[imtstr]
        metadata['output']['ground_motions'][newimt]['max_grid'] = np.max(outgrid[imtstr])
        #
        # How do we get max value on land?
        # Basemap.is_land()?
        #
        metadata['output']['ground_motions'][newimt]['max'] = np.max(outgrid[imtstr])         ################
    metadata['output']['map_information'] = {}
    metadata['output']['map_information']['grid_points'] = {}
    metadata['output']['map_information']['grid_points']['longitude'] = smnx
    metadata['output']['map_information']['grid_points']['latitude'] = smny
    metadata['output']['map_information']['grid_points']['units'] = ''
    metadata['output']['map_information']['grid_spacing'] = {}
    metadata['output']['map_information']['grid_spacing']['longitude'] = smdx
    metadata['output']['map_information']['grid_spacing']['latitude'] = smdy
    metadata['output']['map_information']['grid_spacing']['units'] = 'degrees'
    metadata['output']['map_information']['grid_span'] = {}
    metadata['output']['map_information']['grid_span']['longitude'] = E - W
    metadata['output']['map_information']['grid_span']['latitude'] = N - S
    metadata['output']['map_information']['grid_span']['units'] = 'degrees'
    metadata['output']['map_information']['min'] = {}
    metadata['output']['map_information']['min']['longitude'] = W
    metadata['output']['map_information']['min']['latitude'] = S
    metadata['output']['map_information']['min']['units'] = 'degrees'
    metadata['output']['map_information']['max'] = {}
    metadata['output']['map_information']['max']['longitude'] = E
    metadata['output']['map_information']['max']['latitude'] = N
    metadata['output']['map_information']['max']['units'] = 'degrees'
    metadata['output']['uncertainty'] = {}
    metadata['output']['uncertainty']['grade'] = ''                                     ############# Need map grade
    metadata['output']['uncertainty']['mean_uncertainty'] = {}
    metadata['output']['uncertainty']['total_flagged_mi'] = np.sum(df2['MMI_outliers'] | np.isnan(df2['MMI']))
    all_flagged = np.full(df1['lon'].shape, False, dtype=np.bool)
    for imtstr in imt_in_str_dict['df1']:
        if 'MMI' in imtstr:
            continue
        all_flagged |= df1[imtstr + '_outliers'] | np.isnan(df1[imtstr])
    metadata['output']['uncertainty']['total_flagged_pgm'] = np.sum(all_flagged)
    metadata['processing'] = {}
    metadata['processing']['ground_motion_modules'] = {}
    metadata['processing']['ground_motion_modules']['gmpe'] = {}
    metadata['processing']['ground_motion_modules']['gmpe']['module'] = config['grind']['gmpe']
    metadata['processing']['ground_motion_modules']['gmpe']['reference'] = ''
    metadata['processing']['ground_motion_modules']['ipe'] = {}
    metadata['processing']['ground_motion_modules']['ipe']['module'] = config['ipe_modules'][config['grind']['ipe']][0]
    metadata['processing']['ground_motion_modules']['ipe']['reference'] = ''
    metadata['processing']['ground_motion_modules']['mi2pgm'] = {}             #############3 change these two to just 'gmice'
    metadata['processing']['ground_motion_modules']['mi2pgm']['module'] = config['gmice_modules'][config['grind']['gmice']][0]
    metadata['processing']['ground_motion_modules']['mi2pgm']['reference'] = ''
    metadata['processing']['ground_motion_modules']['pgm2mi'] = {}
    metadata['processing']['ground_motion_modules']['pgm2mi']['module'] = config['gmice_modules'][config['grind']['gmice']][0]
    metadata['processing']['ground_motion_modules']['pgm2mi']['reference'] = ''
    metadata['processing']['ground_motion_modules']['ccf'] = {}                 ############# Add this with HazDev
    metadata['processing']['ground_motion_modules']['ccf']['module'] = config['ccf_modules'][config['grind']['ccf']][0]
    metadata['processing']['ground_motion_modules']['ccf']['reference'] = ''
    metadata['processing']['ground_motion_modules']['basin_correction'] = {} 
    metadata['processing']['ground_motion_modules']['basin_correction']['module'] = 'None'
    metadata['processing']['ground_motion_modules']['basin_correction']['reference'] = ''
    metadata['processing']['ground_motion_modules']['directivity'] = {} 
    metadata['processing']['ground_motion_modules']['directivity']['module'] = 'None'
    metadata['processing']['ground_motion_modules']['directivity']['reference'] = ''
    metadata['processing']['miscellaneous'] = {}
    metadata['processing']['miscellaneous']['bias_max_dsigma'] = bias_max_dsigma
    metadata['processing']['miscellaneous']['bias_max_mag'] = bias_max_mag
    metadata['processing']['miscellaneous']['bias_max_range'] = bias_max_range
    metadata['processing']['miscellaneous']['median_dist'] = 'True'
    metadata['processing']['miscellaneous']['outlier_deviation_level'] = outlier_deviation_level
    metadata['processing']['miscellaneous']['outlier_max_mag'] = outlier_max_mag
    metadata['processing']['shakemap_versions'] = {}
    metadata['processing']['shakemap_versions']['shakemap_revision'] = '4.0a'       ##################
    metadata['processing']['shakemap_versions']['process_time'] = strftime("%Y-%m-%d %H:%M:%S", gmtime())
    metadata['processing']['shakemap_versions']['map_version'] = 1                  ##################
    metadata['processing']['shakemap_versions']['map_status'] = 'released'          ##################
    metadata['processing']['site_response'] = {}
    metadata['processing']['site_response']['vs30default'] = vs30default
    metadata['processing']['site_response']['site_correction'] = 'GMPE native'
    
    oc.addMetadata(metadata)
    
    #
    # Need:
    #   rupture info (how?)
    #   ShakeMap grade (how?)
    #   ShakeMap version, map version
    #   station data stored (how? station.json string?)
    #
    
    for key, value in outgrid.items():
        oc.addData(value, key)
        oc.addData(outsd[key], key + '_sd')
    
    oc.close()
    print('done')
    #------------------------------------------------------------------
    # End grind()
    #------------------------------------------------------------------
    
#%%
if __name__ == '__main__':
    description = '''Process a shakemap...
The only argument is a ShakeMap event ID, which should correspond to a 
directory in the ShakeMap data directory of the current profile.
'''
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('eventid', 
                    help='Path to ShakeMap data directory containing '
                         'input and config files.')
    parser.add_argument('-v','--verbose', action='store_true',
                        help='Print informational messages.')
    pargs = parser.parse_args()
    grind(pargs)


