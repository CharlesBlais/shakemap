#!/usr/bin/env python

import sys
import os.path
import time as time
import argparse
import copy
from importlib import import_module
from time import gmtime, strftime
import json
import warnings

# Not really needed, but handy for looking at the config
#import pprint

import numpy as np
import numpy.ma as ma
import numexpr as ne
from mpl_toolkits.basemap import maskoceans

from openquake.hazardlib import imt
import openquake.hazardlib.const as oqconst
import openquake.hazardlib.gsim.base as oqbase

from shakelib.rupture.point_rupture import PointRupture
from shakelib.sites import Sites
from shakelib.distance import Distance, get_distance
from shakelib.multigmpe import MultiGMPE
from shakelib.virtualipe import VirtualIPE
from shakelib.utils.utils import get_extent
from shakelib.utils.container import InputContainer, OutputContainer

from shakemap.utils.config import get_config_paths

from shakemap._version import get_versions

#%%
#
# helper function for things (ipe, gmice, ccf) that don't have a
# fromConfig() constructor yet. Instantiates an instance of a
# class from config entry, 'name', that has a corresponding
# 'name'_module dictionary of class name, module path
#
def get_object_from_config(obj, cfg, *args):
    cls_abbr = cfg['modeling'][obj]
    mods = obj + '_modules'
    (cname, mpath) = cfg[mods][cls_abbr]
    return getattr(import_module(mpath), cname)(*args)

#
# Set the attributes of the site context
#
def set_sx_params(sx, lons, lats, vs30):
    sx.lons            = lons
    sx.lats            = lats
    sx.vs30            = vs30
    sx.z1pt0_cy14_cal  = Sites._z1pt0_from_vs30_cy14_cal(sx.vs30)
    sx.z1pt0_ask14_cal = Sites._z1pt0_from_vs30_ask14_cal(sx.vs30)
    sx.z2pt5_cb14_cal  = Sites._z2pt5_from_vs30_cb14_cal(sx.vs30) / 1000.0
    sx.z1pt0_cy08      = Sites._z1pt0_from_vs30_cy08(sx.vs30)
    sx.z2pt5_cb07      = Sites._z2pt5_from_z1pt0_cb07(sx.z1pt0_cy08)
    sx.vs30measured    = np.zeros(lons.shape, dtype=bool)
    sx.backarc         = np.zeros(lons.shape, dtype=bool)
    return sx

#
# Helper function to call get_mean_and_stddevs for the
# appropriate object given the IMT
#
def gmas(ipe, gmpe, sx, rx, dx, oqimt, stddev_types):
    if 'MMI' in oqimt:
        pe = ipe
    else:
        pe = gmpe
    return pe.get_mean_and_stddevs(sx, rx, dx, oqimt, stddev_types)

#: Earth radius in km.
EARTH_RADIUS = 6371.0
#
# Quick and dirty distance calculator; uses numexpr for speed
#
def geodetic_distance(lons1, lats1, lons2, lats2, diameter=2*EARTH_RADIUS):
    d = ne.evaluate("EARTH_RADIUS * sqrt(((lons1 - lons2) * cos(0.5 * " \
                    "(lats1 + lats2)))**2.0 + (lats1 - lats2)**2.0)")
    return d

def get_period_index_from_imt_str(imtstr, imt_per_ix):
        if imtstr == 'PGA':
            return imt_per_ix['0.01']
        elif imtstr in ('PGV', 'MMI'):
            return imt_per_ix['1.0']
        else:
            return imt_per_ix[imtstr.replace('SA(', '').replace(')', '')]

def get_period_array(*args):
    imt_per = set()
    for imt_list in args:
        if imt_list is None:
            continue
        for imtstr in imt_list:
            if imtstr == 'PGA':
                imt_per.add(0.01)
            elif imtstr == 'PGV' or imtstr == 'MMI':
                imt_per.add(1.0)
            else:
                imt_per.add(float(imtstr.replace('SA(', '').replace(')', '')))
    return np.array(sorted(imt_per))

def get_imts(imtstr, imtset):

    if imtstr in imtset:
        return (imtstr, )

    salist = [x for x in imtset if x.startswith('SA(')]
    periodlist = [float(x.replace('SA(', '').replace(')', '')) for x in salist]
    periodlist = sorted(periodlist)
    periodlist_str = [str(x) for x in periodlist]

    #
    # If we're here, then we know that IMT isn't in the inputs. Try
    # some alternatives.
    #
    if imtstr == 'PGA':
        #
        # Use the highest frequency in the inputs, otherwise use PGV
        #
        if len(salist):
            return ('SA(' + periodlist_str[0] + ')', )
        elif 'PGV' in imtset:
            return ('PGV', )
        else:
            return ()
    elif imtstr == 'PGV':
        #
        # Use 1.0 sec SA (or its bracket) if it's there, otherwise
        # use PGA
        #
        sa_tuple = get_sa_bracket(1.0, periodlist, periodlist_str)
        if sa_tuple != ():
            return sa_tuple
        if 'PGA' in imtset:
            return ('PGA', )
        else:
            return ()
    elif imtstr == 'MMI':
        #
        # Use PGV if it's there, otherwise use 1.0 sec SA (or its
        # bracket)
        #
        if 'PGV' in imtset:
            return ('PGV', )
        return get_sa_bracket(1.0, periodlist, periodlist_str)
    elif imtstr.startswith('SA('):
        myper = float(imtstr.replace('SA(', '').replace(')', ''))
        return get_sa_bracket(myper, periodlist, periodlist_str)
    else:
        raise ValueError('Unknown IMT %s in get_imt_bracket' % imtstr)

def get_sa_bracket(myper, plist, plist_str):

    if not len(plist):
        return ()
    try:
        return ('SA(' + plist_str[plist.index(myper)] + ')', )
    except ValueError:
        pass
    for i, v in enumerate(plist):
        if v > myper:
            break
    if i == 0 or v < myper:
        return ('SA(' + plist_str[i] + ')', )
    else:
        return ('SA(' + plist_str[i-1] + ')', 'SA(' + plist_str[i] + ')')

def get_sta_imts(imtstr, sdf, ix, imtset):
    myimts = set()
    for this_imt in imtset:
        if not np.isnan(sdf[this_imt][ix]) and \
           not sdf[this_imt + '_outliers'][ix]:
            myimts.add(this_imt)
    return get_imts(imtstr, myimts)

def get_map_grade(do_grid, outsd, psd, moutgrid):
    mean_rat = '-'
    mygrade = '-'
    if not do_grid or 'PGA' not in outsd:
        return mean_rat, mygrade, []
    sd_rat = outsd['PGA'] / psd['PGA']
    mmimasked = ma.masked_less(moutgrid['MMI'], 6.0)
    mpgasd_rat = ma.masked_array(sd_rat, mask=mmimasked.mask)
    if not np.all(mpgasd_rat.mask):
        gvals = [0.96, 0.98, 1.05, 1.25]
        grades = ['A', 'B', 'C', 'D', 'F']
        mean_rat = mpgasd_rat.mean()
        for ix, val in enumerate(gvals):
            if mean_rat <= val:
                mygrade = grades[ix]
                break
        if mygrade == '-':
            mygrade = 'F'
    return mean_rat, mygrade, sd_rat

#%%
#args = type('Dummy', (object,), {'eventid' : 'wenchuan',
#                                 'verbose' : True})
#if True:
def interp(args):
    verbose = args.verbose
    #
    # Find the shake_data file
    #
    install_path, data_path = get_config_paths()
    datadir = os.path.join(data_path, args.eventid, 'current')
    if not os.path.isdir(datadir):
        print('%s is not a valid directory.' % datadir)
        sys.exit(1)
    datafile = os.path.join(datadir, 'shake_data.hdf')
    if not os.path.isfile(datafile):
        print('%s is not a valid shake data file.' % datafile)
        sys.exit(1)
    #------------------------------------------------------------------
    # Make the input container and extract the config
    #------------------------------------------------------------------
    ic = InputContainer.loadFromHDF(datafile)
    config = ic.getConfig()
    #------------------------------------------------------------------
    # Instantiate the gmpe, gmice, ipe, and ccf
    #------------------------------------------------------------------
    gmpe = MultiGMPE.from_config(config)

    gmice = get_object_from_config('gmice', config)

    if config['ipe_modules'][config['modeling']['ipe']][0] == 'VirtualIPE':
        ipe = VirtualIPE.fromFuncs(gmpe, gmice)
    else:
        ipe = get_object_from_config('ipe', config)
    #------------------------------------------------------------------
    # Bias parameters
    #------------------------------------------------------------------
    do_bias         = config['modeling']['bias']['do_bias']
    bias_max_range  = config['modeling']['bias']['max_range']
    bias_max_mag    = config['modeling']['bias']['max_mag']
    bias_max_dsigma = config['modeling']['bias']['max_delta_sigma']
    #------------------------------------------------------------------
    # Outlier parameters
    #------------------------------------------------------------------
    outlier_deviation_level = config['data']['outlier']['max_deviation']
    outlier_max_mag = config['data']['outlier']['max_mag']
    #------------------------------------------------------------------
    # These are the IMTs we want to make
    #------------------------------------------------------------------
    imt_out_set_str = set(config['interp']['imt_list'])
    #------------------------------------------------------------------
    # Get the rupture object and rupture context
    #------------------------------------------------------------------
    rupture_obj = ic.getRupture()
    rx = rupture_obj.getRuptureContext([gmpe])
    if rx.rake == None:
        rx.rake = 0
    #------------------------------------------------------------------
    # Get the Vs30 file name
    #------------------------------------------------------------------
    vs30default = config['data']['vs30default']
    vs30_file = config['data']['vs30file']
    if not vs30_file:
        vs30_file = None
    #------------------------------------------------------------------
    # The output locations: either a grid or a list of points
    #------------------------------------------------------------------
    smdx = config['interp']['prediction_location']['xres']
    smdy = config['interp']['prediction_location']['yres']
    if config['interp']['prediction_location']['file'] and \
       config['interp']['prediction_location']['file'] != 'None':
        #
        # FILE: Open the file and get the output points
        #
        do_grid = False
        in_sites = np.genfromtxt(
                        config['interp']['prediction_location']['file'],
                        autostrip=True, unpack=True,
                        dtype=[np.float, np.float, np.float, '<U80'])
        lons, lats, vs30, idents = zip(*in_sites)
        lons = np.array(lons).reshape(1, -1)
        lats = np.array(lats).reshape(1, -1)
        vs30 = np.array(vs30).reshape(1, -1)
        depths = np.zeros_like(lats)
        W = np.min(lons)
        E = np.max(lons)
        S = np.min(lats)
        N = np.max(lats)
        smnx = np.size(lons)
        smny = 1
        dist_obj_out = Distance(gmpe, lons, lats, depths, rupture_obj)

        sites_obj_out = Sites.fromBounds(W, E, S, N, smdx, smdy,
                                         defaultVs30=vs30default,
                                         vs30File=vs30_file)

        sx_out_soil = oqbase.SitesContext()
#        sx_out_rock = oqbase.SitesContext()

        sx_out_soil = set_sx_params(sx_out_soil, lons, lats, vs30)
#        vs30_rock = np.full_like(lons, vs30default)
#        sx_out_rock = set_sx_params(sx_out_rock, lons, lats, vs30_rock)
    else:
        #
        # GRID: Figure out the grid parameters and get output points
        #
        do_grid = True
        smdx = config['interp']['prediction_location']['xres']
        smdy = config['interp']['prediction_location']['yres']

        if config['interp']['prediction_location']['extent']:
           W, S, E, N = config['interp']['prediction_location']['extent']
        else:
           W, E, S, N = get_extent(rupture_obj)

        sites_obj_out = Sites.fromBounds(W, E, S, N, smdx, smdy,
                                         defaultVs30=vs30default,
                                         vs30File=vs30_file)
        smnx, smny = sites_obj_out.getNxNy()

        sx_out_soil = sites_obj_out.getSitesContext()
#        sx_out_rock = sites_obj_out.getSitesContext(rock_vs30=vs30default)
        lons, lats = np.meshgrid(sx_out_soil.lons, sx_out_soil.lats)
        lons = np.flipud(lons)
        lats = np.flipud(lats)
        lons = lons.flatten()
        lats = lats.flatten()
        depths = np.zeros_like(lats)

        dist_obj_out = Distance.fromSites(gmpe, sites_obj_out, rupture_obj)

    dx_out = dist_obj_out.getDistanceContext()

    lons_out_rad = np.radians(lons)
    lats_out_rad = np.radians(lats)
    #------------------------------------------------------------------
    # Station data
    #------------------------------------------------------------------
    stations = ic.getStationList()
    stddev_types = [oqconst.StdDev.TOTAL, oqconst.StdDev.INTER_EVENT,
                    oqconst.StdDev.INTRA_EVENT]
    #
    # df1 holds the instrumented data (PGA, PGV, SA)
    # df2 holds the non-instrumented data (MMI)
    #
    df_dict = {'df1': None, 'df2': None}
    imt_in_str_dict = {'df1': None, 'df2': None}
    imt_in_dict = {'df1': None, 'df2': None}
    imt_in_str_set = set()
    sx_dict = {'df1': None, 'df2': None}
    dx_dict = {'df1': None, 'df2': None}
    if stations is not None:
        df_dict['df1'] = stations.getStationDataframe(1)
        df_dict['df2'] = stations.getStationDataframe(0)
        #
        # Get the sites and distance contexts for each dataframe then
        # compute the predictions for the IMTs in that dataframe.
        #
        for ndf, df in df_dict.items():
            if not df:
                imt_in_str_dict[ndf] = set()
                imt_in_dict[ndf] = set()
                continue
            #
            # Get lists of the input IMTs
            #
            imt_in_str_dict[ndf] = set(
                    [x for x in df.keys() if x in ('PGA', 'PGV', 'MMI') or
                     x.startswith('SA(')])
            imt_in_dict[ndf] = set(
                    [imt.from_string(x) for x in imt_in_str_dict[ndf]])
            imt_in_str_set |= imt_in_str_dict[ndf]
            #
            # Get the sites and distance contexts
            #
            df['depth'] = np.zeros_like(df['lon'])
            lldict = {'lons': df['lon'], 'lats': df['lat']}
            sx_dict[ndf] = sites_obj_out.getSitesContext(lldict)
            dist_obj = Distance(gmpe, df['lon'], df['lat'], df['depth'],
                                rupture_obj)
            dx_dict[ndf] = dist_obj.getDistanceContext()
            #
            # Do the predictions and other bookkeeping for each IMT
            #
            for imtstr in imt_in_str_dict[ndf]:
                pmean, pstddev = gmas(ipe, gmpe, sx_dict[ndf], rx,
                                      dx_dict[ndf], imt.from_string(imtstr),
                                      stddev_types)
                df[imtstr + '_pred'] = pmean
                df[imtstr + '_pred_sd_total'] = pstddev[0]
                df[imtstr + '_pred_sd_inter'] = pstddev[1]
                df[imtstr + '_pred_sd_intra'] = pstddev[2]
                #
                # Compute the total residual
                #
                df[imtstr + '_residual'] = df[imtstr] - df[imtstr + '_pred']
                #
                # Do the outlier flagging if we don't have a fault and
                # the event magnitude is over the limit
                #
                if not isinstance(rupture_obj, PointRupture) or \
                   rx.mag <= outlier_max_mag:
                    #
                    # turn off nan warnings for this statement
                    #
                    np.seterr(invalid='ignore')
                    flagged = np.abs(df[imtstr + '_residual']) > \
                              outlier_deviation_level * \
                              df[imtstr + '_pred_sd_total']
                    np.seterr(invalid='warn')
                    if verbose:
                        print('IMT: %s, flagged: %d' %
                              (imtstr, np.sum(flagged)))
                    df[imtstr + '_outliers'] = flagged
                else:
                    df[imtstr + '_outliers'] = np.full(df[imtstr].shape, True,
                                                       dtype=np.bool)
                #
                # Make the uncertainty arrays for any IMTs that don't
                # have them.
                #
                if (imtstr + '_sd') not in df:
                    if imtstr == 'MMI':
                        sdval = 0.3
                    else:
                        sdval = 0.0
                    df[imtstr + '_sd'] = np.full_like(df['lon'], sdval)
            #
            # Get the lons/lats in radians while we're at it
            #
            df['lon_rad'] = np.radians(df['lon'])
            df['lat_rad'] = np.radians(df['lat'])
            #
            # It will be handy later on to have the rupture distance
            # in the dataframes
            #
            dd = get_distance(['rrup'], df['lat'], df['lon'],
                                      df['depth'], rupture_obj)
            df['rrup'] = dd['rrup']
    df1 = df_dict['df1']
    df2 = df_dict['df2']

#%%
    #------------------------------------------------------------------
    # Compute all the IMTs possible from MMI
    # This logic needs to be revisited. We should probably make what
    # we have to to do the CMS to make the needed output IMTs, but
    # for now, we're just going to use what we have and the ccf.
    #------------------------------------------------------------------
    if df2:
        for gmice_imt in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            if imt.SA == gmice_imt:
                iterlist = gmice.DEFINED_FOR_SA_PERIODS
            else:
                iterlist = [None]
            for period in iterlist:
                oqimt = gmice_imt(period)
                imtstr = str(oqimt)

                np.seterr(invalid='ignore')
                df2[imtstr], _ = gmice.getGMfromMI(df2['MMI'], oqimt,
                                                   dists=df2['rrup'],
                                                   mag=rx.mag)
                df2[imtstr][df2['MMI'] < 4.0] = np.nan
                np.seterr(invalid='warn')
                df2[imtstr + '_sd'] = np.full_like(df2['MMI'],
                       gmice.getMI2GMsd()[oqimt])
                imt_in_str_dict['df2'].add(imtstr)
                #
                # Get the predictions and stddevs, too
                #
                pmean, pstddev = gmas(ipe, gmpe, sx_dict['df2'], rx,
                                      dx_dict['df2'], oqimt, stddev_types)
                df2[imtstr + '_pred'] = pmean
                df2[imtstr + '_pred_sd_total'] = pstddev[0]
                df2[imtstr + '_pred_sd_inter'] = pstddev[1]
                df2[imtstr + '_pred_sd_intra'] = pstddev[2]
                df2[imtstr + '_residual'] = df2[str(oqimt)] - pmean
                df2[imtstr + '_outliers'] = np.full(pmean.shape, False,
                                                    dtype=np.bool)

    #
    # Now make derived MMI from the best available PGM; This is ugly and it
    # would be nice to have a more deterministic way of doing it
    #
    if df1:
        imtstr = None
        if 'PGV' in df1 \
                and imt.PGV in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            imtstr = 'PGV'
        elif 'PGA' in df1 \
                and imt.PGA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES:
            imtstr = 'PGA'
        elif 'SA(1.0)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 1.0 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(1.0)'
        elif 'SA(0.3)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 0.3 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(0.3)'
        elif 'SA(3.0)' in df1 \
                and imt.SA in gmice.DEFINED_FOR_INTENSITY_MEASURE_TYPES \
                and 3.0 in gmice.DEFINED_FOR_SA_PERIODS:
            imtstr = 'SA(3.0)'

        if imtstr is not None:
            oqimt = imt.from_string(imtstr)
            np.seterr(invalid='ignore')
            df1['MMI'], _ = gmice.getMIfromGM(df1[imtstr], oqimt,
                                              dists=df1['rrup'], mag=rx.mag)
            np.seterr(invalid='warn')
            df1['MMI_sd'] = np.full_like(df1[imtstr],
                                         gmice.getGM2MIsd()[oqimt])
            imt_in_str_dict['df1'].add('MMI')
            #
            # Get the prediction and stddevs
            #
            pmean, pstddev = gmas(ipe, gmpe, sx_dict['df1'], rx, dx_dict['df1'],
                                  imt.from_string('MMI'), stddev_types)
            df1['MMI' + '_pred'] = pmean
            df1['MMI' + '_pred_sd_total'] = pstddev[0]
            df1['MMI' + '_pred_sd_inter'] = pstddev[1]
            df1['MMI' + '_pred_sd_intra'] = pstddev[2]
            df1['MMI' + '_residual'] = df1['MMI'] - pmean
            df1['MMI' + '_outliers'] = np.full(pmean.shape, False,
                                               dtype=np.bool)

    imt_per = get_period_array(imt_in_str_dict['df1'],
                               imt_in_str_dict['df2'],
                               imt_out_set_str)
    ccf = get_object_from_config('ccf', config, imt_per)

    imt_per_ix = {str(per) : ix for ix, per in enumerate(imt_per)}
#%%
    #------------------------------------------------------------------
    # Do the MVN
    #------------------------------------------------------------------
    #
    # First do the bias for all of the input and output IMTs. Hold on
    # to some of the products that will be used for the interpolation.
    #
    bias = {}
    sigma_2_eta = {}
    outgrid = {}
    outsd = {}
    psd = {}

    sta_lons_rad = {}
    sta_lats_rad = {}
    sta_resids = {}
    sta_period_ix = {}
    sta_sigs = {}
    sta_sigs_total = {}
    sta_sigs_inter = {}
    corr_adj = {}
    sigma22inv = {}

    #
    # Python scoping is weird. But ths works. Seemingly.
    # The only reason to have this function is to avoid repeating
    # all this code, and the reason to make it nested is to avoid
    # having to pass eleven arguments.
    #
    def build_list(imtstr, imtin, sdf, vidx):
        inperiod_ix = get_period_index_from_imt_str(imtin, imt_per_ix)
        sta_lons_rad[imtstr] = np.append(sta_lons_rad[imtstr],
                                         sdf['lon_rad'][vidx])
        sta_lats_rad[imtstr] = np.append(sta_lats_rad[imtstr],
                                         sdf['lat_rad'][vidx])
        sta_resids[imtstr] = np.append(sta_resids[imtstr],
                               sdf[imtin + '_residual'][vidx])
        sdarr = sdf[imtin + '_pred_sd_intra'][vidx]
        sta_sigs[imtstr] = np.append(sta_sigs[imtstr], sdarr)
        sta_sigs_inter[imtstr] = np.append(sta_sigs_inter[imtstr],
                      sdf[imtin + '_pred_sd_inter'][vidx])
        sta_sigs_total[imtstr] = np.append(sta_sigs_total[imtstr],
                      np.sqrt(sdarr**2 + sdf[imtin + '_sd'][vidx]**2))
        if isinstance(vidx, int):
            per_val = inperiod_ix
        else:
            per_val = np.full(sdarr.shape, inperiod_ix, dtype=np.int64)
        sta_period_ix[imtstr] = np.append(sta_period_ix[imtstr], per_val)
        return

    #
    # Compute a bias for all of the IMTs in the inputs and outputs
    #
    combined_imt_str = imt_out_set_str
    if imt_in_str_dict['df1']:
        combined_imt_str |= imt_in_str_dict['df1']
    if imt_in_str_dict['df2']:
        combined_imt_str |= imt_in_str_dict['df2']
    for imtstr in combined_imt_str:
        time1 = time.time()
        #
        # Get the index of the (pesudo-) period of the output IMT
        #
        outperiod_ix = get_period_index_from_imt_str(imtstr, imt_per_ix)
        #
        # Fill the station arrays
        #
        sta_lons_rad[imtstr] = np.array([])
        sta_lats_rad[imtstr] = np.array([])
        sta_resids[imtstr] = np.array([])
        sta_sigs[imtstr] = np.array([])
        sta_sigs_total[imtstr] = np.array([])
        sta_sigs_inter[imtstr] = np.array([])
        sta_period_ix[imtstr] = np.array([], dtype=np.int64)
        for ndf, sdf in df_dict.items():
            if not sdf:
                continue
            if ndf == 'df1':
                for i in range(np.size(sdf['lon'])):
                    for imtin in get_sta_imts(imtstr, sdf, i,
                                              imt_in_str_dict[ndf]):
                        build_list(imtstr, imtin, sdf, i)
            else:
                for imtin in get_imts(imtstr, imt_in_str_dict['df2']):
                    vidx = ~(np.isnan(sdf[imtin]) | sdf[imtin + '_outliers'])
                    build_list(imtstr, imtin, sdf, vidx)
        if np.size(sta_lons_rad[imtstr]) == 0:
            bias[imtstr] = 0.0
            sigma_2_eta[imtstr] = 0.0
            continue
        sta_lons_rad[imtstr] = sta_lons_rad[imtstr].reshape((-1, 1))
        sta_lats_rad[imtstr] = sta_lats_rad[imtstr].reshape((-1, 1))
        sta_resids[imtstr] = sta_resids[imtstr].reshape((-1, 1))
        sta_sigs[imtstr] = sta_sigs[imtstr].reshape((-1, 1))
        sta_sigs_total[imtstr] = sta_sigs_total[imtstr].reshape((-1, 1))
        sta_period_ix[imtstr] = sta_period_ix[imtstr].reshape((-1, 1))
        corr_adj[imtstr] = (sta_sigs[imtstr] / sta_sigs_total[imtstr])
        corr_adj22 = corr_adj[imtstr] * corr_adj[imtstr].T
        np.fill_diagonal(corr_adj22, 1.0)
        dist22 = geodetic_distance(sta_lons_rad[imtstr],
                                   sta_lats_rad[imtstr],
                                   sta_lons_rad[imtstr].T,
                                   sta_lats_rad[imtstr].T)
        d22_rows, d22_cols = np.shape(dist22) # should be square
        t1_22 = np.tile(sta_period_ix[imtstr], (1, d22_cols))
        t2_22 = np.tile(sta_period_ix[imtstr].T, (d22_rows, 1))
        corr22 = ccf.getCorrelation(t1_22, t2_22, dist22) * corr_adj22
        sigma22 = corr22 * (sta_sigs[imtstr] * sta_sigs[imtstr].T)
        sigma22inv[imtstr] = np.linalg.pinv(sigma22)
        #
        # Compute the bias and apply it to the predictions and the
        # residuals
        #
        if do_bias and (not isinstance(rupture_obj, PointRupture)
                        or rx.mag <= bias_max_mag):
            ONE = np.ones_like(sta_resids[imtstr])
            tau = np.mean(sta_sigs_inter[imtstr])
            sigma_2_eta[imtstr] = 1.0 / (1 / tau**2 + \
                                   ONE.T.dot(sigma22inv[imtstr].dot(ONE)))
            bias[imtstr] = \
                    ONE.T.dot(sigma22inv[imtstr].dot(sta_resids[imtstr])) * \
                    sigma_2_eta[imtstr]
            if bias[imtstr] > bias_max_dsigma:
                bias[imtstr] = 0.0
                sigma_2_eta[imtstr] = 0.0
        else:
            bias[imtstr] = 0.0
            sigma_2_eta[imtstr] = 0.0
        bias_time = time.time() - time1
        if verbose:
            print('%s: bias %f stddev %f; %d stations (time=%f sec)' %
                  (imtstr, bias[imtstr], np.sqrt(sigma_2_eta[imtstr]),
                   np.size(sta_lons_rad[imtstr]), bias_time))
    #
    # End bias
    #
    #
    # Now do the MVN with the intra-event residuals
    #
    for imtstr in imt_out_set_str:
        time1 = time.time()
        #
        # Get the index of the (pesudo-) period of the output IMT
        #
        outperiod_ix = get_period_index_from_imt_str(imtstr, imt_per_ix)
        #
        # Get the predictions at the output points
        #
        oqimt = imt.from_string(imtstr)
        pout_mean, pout_sd = gmas(ipe, gmpe, sx_out_soil, rx, dx_out, oqimt,
                                  stddev_types)
        if bias[imtstr]:
            psd[imtstr] = pout_sd[2]
        else:
            psd[imtstr] = pout_sd[0]
        pout_sd2 = np.power(psd[imtstr], 2.0)

        #
        # Bias the predictions
        #
        pout_mean += bias[imtstr]
        #
        # If there are no data, just use the unbiased prediction
        #
        if np.size(sta_lons_rad[imtstr]) == 0:
            outgrid[imtstr] = pout_mean
            outsd[imtstr] = pout_sd[0]
            continue
        #
        # Remake the residual array now that we (may) have a bias
        #
        if bias[imtstr] != 0:
            sta_resids[imtstr] = np.array([])
            sdf = df_dict['df1']
            if sdf:
                for i in range(np.size(sdf['lon'])):
                    for imtin in get_sta_imts(imtstr, sdf, i,
                                              imt_in_str_dict['df1']):
                        sta_resids[imtstr] = np.append(
                                sta_resids[imtstr],
                                sdf[imtin + '_residual'][i] - bias[imtin])
            sdf = df_dict['df2']
            if sdf:
                for imtin in get_imts(imtstr, imt_in_str_dict['df2']):
                    vidx = ~(np.isnan(sdf[imtin]) | sdf[imtin + '_outliers'])
                    sta_resids[imtstr] = np.append(
                            sta_resids[imtstr],
                            sdf[imtin + '_residual'][vidx] - bias[imtin])
        sta_resids[imtstr] = sta_resids[imtstr].reshape((-1, 1))
        #
        # Now do the MVN itself...
        #
        dtime = 0
        mtime = 0
        ddtime = 0
        ctime = stime = atime = 0

        ampgrid = np.zeros_like(pout_mean)
        sdgrid = np.zeros_like(pout_mean)
        corr_adj12 = corr_adj[imtstr] * np.ones((1, smnx))
        for iy in range(smny):
            ss = iy * smnx
            se = (iy + 1) * smnx
            time4 = time.time()
            dist12 = geodetic_distance(lons_out_rad[ss:se].reshape(1, -1),
                                       lats_out_rad[ss:se].reshape(1, -1),
                                       sta_lons_rad[imtstr],
                                       sta_lats_rad[imtstr])
            t2_12 = np.full(dist12.shape, outperiod_ix, dtype=np.int)
            d12_rows, d12_cols = np.shape(dist12)
            t1_12 = np.tile(sta_period_ix[imtstr], (1, d12_cols))
            ddtime += time.time() - time4
            time4 = time.time()
            corr12 = ccf.getCorrelation(t1_12, t2_12, dist12)
            ctime += time.time() - time4
            time4 = time.time()
            sdarr = psd[imtstr][iy, :].reshape((1, -1))
            ss = sta_sigs[imtstr]
            sigma12 =  ne.evaluate("corr12 * corr_adj12 * (ss * sdarr)").T
            stime += time.time() - time4
            time4 = time.time()
            rcmatrix = sigma12.dot(sigma22inv[imtstr])
            dtime += time.time() - time4
            time4 = time.time()
            ampgrid[iy, :] = pout_mean[iy, :] + ((corr_adj12.T *
                   rcmatrix).dot(sta_resids[imtstr])).reshape((-1,))
            atime += time.time() - time4
            time4 = time.time()
    #        sdgrid[ss:se] = pout_sd2[ss:se] - np.diag(rcmatrix.dot(sigma12))
            sdgrid[iy, :] = pout_sd2[iy, :] - \
                    np.sum(rcmatrix * sigma12, axis=1)
            mtime += time.time() - time4

        outgrid[imtstr] = ampgrid
        sdgrid[sdgrid < 0] = 0
        outsd[imtstr] = np.sqrt(sdgrid)
        if verbose:
            print('\ttime for %s distance=%f' % (imtstr, ddtime))
            print('\ttime for %s correlation=%f' % (imtstr, ctime))
            print('\ttime for %s sigma=%f' % (imtstr, stime))
            print('\ttime for %s rcmatrix=%f' % (imtstr, dtime))
            print('\ttime for %s amp calc=%f' % (imtstr, atime))
            print('\ttime for %s sd calc=%f' % (imtstr, mtime))
            print('total time for %s=%f' % (imtstr, time.time() - time1))

#%%
    #------------------------------------------------------------------
    # Output the data and metadata
    #------------------------------------------------------------------
    product_path = os.path.join(datadir, 'products')
    if not os.path.isdir(product_path):
        os.mkdir(product_path)
    oc = OutputContainer.createEmpty(os.path.join(product_path,
                                                  'shake_result.hdf'))
    #
    # Might as well stick the whole config in the result
    #
    oc.addMetadata(config, name='config')

    #
    # We're going to need masked arrays of the output grids later, so
    # make them now. We only need to make the mask once, then we can
    # apply it to all of the other grids.
    #
    moutgrid = {}
    refimt = list(imt_out_set_str)[0]
    #
    # Don't know what to do about this warning; hopefully someone
    # will fix maskoceans()
    #
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
        moutgrid[refimt] = maskoceans(lons.reshape((smny, smnx)),
                                      lats.reshape((smny, smnx)),
                                      outgrid[refimt], inlands=False, grid=1.25)
    for imtout in imt_out_set_str:
        if imtout == refimt:
            continue
        moutgrid[imtout] = ma.masked_array(outgrid[imtout],
                mask=copy.copy(moutgrid[refimt].mask))
    #
    # Get the map grade
    #
    mean_rat, mygrade, sd_rat = get_map_grade(do_grid, outsd, psd, moutgrid)
    #
    # This is the metadata for creating info.json
    #
#
# Need:
#   map version, map status
#
    ip = 'input'
    ei = 'event_information'
    op = 'output'
    gm = 'ground_motions'
    mi = 'map_information'
    un = 'uncertainty'
    pp = 'processing'
    gmm = 'ground_motion_modules'
    ms = 'miscellaneous'
    sv = 'shakemap_versions'
    sr = 'site_response'
    info = {}
    info[ip] = {}
    info[ip][ei] = {}
    info[ip][ei]['depth'] = str(rx.hypo_depth)
    info[ip][ei]['event_id'] = args.eventid
    info[ip][ei]['fault_ref'] = rupture_obj.getReference()
#    info[ip][ei]['faultfiles'] = ''                             #### Do we need this? I don't think so.
    info[ip][ei]['feregion'] = config['zone_info']['fename']
    info[ip][ei]['intensity_observations'] = str(np.size(df2['lon'])) if df2 else '0'
    info[ip][ei]['latitude'] = str(rx.hypo_lat)
    info[ip][ei]['longitude'] = str(rx.hypo_lon)
    info[ip][ei]['location'] = rupture_obj._origin.locstring
    info[ip][ei]['magnitude'] = str(rx.mag)
    info[ip][ei]['mech_src'] = config['zone_info']['moment_tensor_source']
    info[ip][ei]['origin_time'] = rupture_obj._origin.time.strftime('%Y-%m-%d %H:%M:%S')
    info[ip][ei]['seismic_stations'] = str(np.size(df1['lon'])) if df1 else '0'
    info[ip][ei]['src_mech'] = rupture_obj._origin.mech
    info[ip][ei]['tectonic_regime'] = config['zone_info']['domain']
    info[ip][ei]['event_description'] = rupture_obj._origin.locstring  ##### This AND locaction?
    info[ip][ei]['event_type'] = rupture_obj._origin.mech              ##### This AND src_mech?
    info[op] = {}
    info[op][gm] = {}
    for myimt in imt_out_set_str:
        info[op][gm][myimt] = {}
        if myimt == 'MMI':
            units = 'intensity'
        elif myimt == 'PGV':
            units = 'cms'
        else:
            units = 'ln(g)'
        info[op][gm][myimt]['units'] = units
        info[op][gm][myimt]['bias'] = str(bias[myimt]) if myimt in bias else '-'
        info[op][gm][myimt]['max_grid'] = str(np.max(outgrid[myimt]))
        info[op][gm][myimt]['max'] = str(np.max(moutgrid[myimt]))
    info[op][mi] = {}
    info[op][mi]['grid_points'] = {}
    info[op][mi]['grid_points']['longitude'] = str(smnx)
    info[op][mi]['grid_points']['latitude'] = str(smny)
    info[op][mi]['grid_points']['units'] = ''
    info[op][mi]['grid_spacing'] = {}
    info[op][mi]['grid_spacing']['longitude'] = str(smdx)
    info[op][mi]['grid_spacing']['latitude'] = str(smdy)
    info[op][mi]['grid_spacing']['units'] = 'degrees'
    info[op][mi]['grid_span'] = {}
    info[op][mi]['grid_span']['longitude'] = str(E - W)
    info[op][mi]['grid_span']['latitude'] = str(N - S)
    info[op][mi]['grid_span']['units'] = 'degrees'
    info[op][mi]['min'] = {}
    info[op][mi]['min']['longitude'] = str(W)
    info[op][mi]['min']['latitude'] = str(S)
    info[op][mi]['min']['units'] = 'degrees'
    info[op][mi]['max'] = {}
    info[op][mi]['max']['longitude'] = str(E)
    info[op][mi]['max']['latitude'] = str(N)
    info[op][mi]['max']['units'] = 'degrees'
    info[op][un] = {}
    info[op][un]['grade'] = mygrade
    info[op][un]['mean_uncertainty_ratio'] = mean_rat
    info[op][un]['total_flagged_mi'] = str(np.sum(df2['MMI_outliers'] | np.isnan(df2['MMI']))) if df2 else '0'
    if df1:
        all_flagged = np.full(df1['lon'].shape, False, dtype=np.bool)
        for imtstr in imt_in_str_dict['df1']:
            if 'MMI' in imtstr:
                continue
            all_flagged |= df1[imtstr + '_outliers'] | np.isnan(df1[imtstr])
        info[op][un]['total_flagged_pgm'] = str(np.sum(all_flagged))
    else:
        info[op][un]['total_flagged_pgm'] = '0'
    info[pp] = {}
    info[pp][gmm] = {}
    info[pp][gmm]['gmpe'] = {}
    info[pp][gmm]['gmpe']['module'] = str(config['modeling']['gmpe'])
    info[pp][gmm]['gmpe']['reference'] = ''
    info[pp][gmm]['ipe'] = {}
    info[pp][gmm]['ipe']['module'] = str(config['ipe_modules'][config['modeling']['ipe']][0])
    info[pp][gmm]['ipe']['reference'] = ''
    info[pp][gmm]['gmice'] = {}
    info[pp][gmm]['gmice']['module'] = str(config['gmice_modules'][config['modeling']['gmice']][0])
    info[pp][gmm]['gmice']['reference'] = ''
    info[pp][gmm]['ccf'] = {}
    info[pp][gmm]['ccf']['module'] = str(config['ccf_modules'][config['modeling']['ccf']][0])
    info[pp][gmm]['ccf']['reference'] = ''
    info[pp][gmm]['basin_correction'] = {}
    info[pp][gmm]['basin_correction']['module'] = 'None'
    info[pp][gmm]['basin_correction']['reference'] = ''
    info[pp][gmm]['directivity'] = {}
    info[pp][gmm]['directivity']['module'] = 'None'
    info[pp][gmm]['directivity']['reference'] = ''
    info[pp][ms] = {}
    info[pp][ms]['bias_max_dsigma'] = str(bias_max_dsigma)
    info[pp][ms]['bias_max_mag'] = str(bias_max_mag)
    info[pp][ms]['bias_max_range'] = str(bias_max_range)
    info[pp][ms]['median_dist'] = 'True'
    info[pp][ms]['outlier_deviation_level'] = str(outlier_deviation_level)
    info[pp][ms]['outlier_max_mag'] = str(outlier_max_mag)
    info[pp][sv] = {}
    info[pp][sv]['shakemap_revision'] = get_versions()['version']
    info[pp][sv]['shakemap_revision_id'] = get_versions()['full-revisionid']
    info[pp][sv]['process_time'] = strftime("%Y-%m-%d %H:%M:%S%Z", gmtime())
    info[pp][sv]['map_version'] = ic.getHistory()['history'][-1][2]
    info[pp][sv]['map_data_history'] = ic.getHistory()['history']
    info[pp][sv]['map_status'] = config['system']['map_status']
    info[pp][sr] = {}
    info[pp][sr]['vs30default'] = str(vs30default)
    info[pp][sr]['site_correction'] = 'GMPE native'

    oc.addData(json.dumps(info), 'info.json')

    #
    # Add the rupture JSON as a text string
    #
    oc.addData(json.dumps(rupture_obj._geojson), 'rupture.json')

    #
    # Add the station data. The stationlist object has the original
    # data and produces a GeoJSON object (a dictionary, really), but
    # we need to add peak values and flagging that has been done here.
    #
    if stations:
        sjdict = stations.getGeoJson()
        sta_ix = {}
        for ndf, sdf in df_dict.items():
            if not sdf:
                sta_ix[ndf] = {}
            else:
                sta_ix[ndf] = dict(zip(sdf['id'], range(len(sdf['id']))))
        for station in sjdict['features']:
            if station['id'] in sta_ix['df1']:
                sdf = df1
                six = sta_ix['df1'][station['id']]
            elif station['id'] in sta_ix['df2']:
                sdf = df2
                six = sta_ix['df2'][station['id']]
            else:
                raise ValueError('Unknown station %s in stationlist' %
                                (station['id']))
            if 'MMI' in sdf and not sdf['MMI_outliers'][six]:
                station['properties']['intensity'] = '%.1f' % sdf['MMI'][six]
            else:
                station['properties']['intensity'] = 'null'

            if 'PGA' in sdf and not sdf['PGA_outliers'][six]:
                station['properties']['pga'] = '%.4f' % \
                                            (np.exp(sdf['PGA'][six]) * 100)
            else:
                station['properties']['pga'] = 'null'
            if 'PGV' in sdf and not sdf['PGV_outliers'][six]:
                station['properties']['pgv'] = '%.4f' % (np.exp(sdf['PGV'][six]))
            else:
                station['properties']['pgv'] = 'null'
            station['properties']['distance'] = '%.2f' % sdf['rrup'][six]
            for channel in station['properties']['channels']:
                for amp in channel['amplitudes']:
                    Name = amp['name'].upper()
                    if sdf[Name + '_outliers'][six]:
                        if amp['flag'] == '0':
                            amp['flag'] = 'T'
                        else:
                            amp['flag'] += 'T'
                    if not amp['flag']:
                        amp['flag'] = '0'
    else:
        sjdict = {}

    oc.addData(json.dumps(sjdict), 'stationlist.json')

    metadata = {}
    if do_grid:
        metadata['type'] = 'grid'
        metadata['W'] = W
        metadata['E'] = E
        metadata['S'] = S
        metadata['N'] = N
        metadata['nx'] = smnx
        metadata['ny'] = smny
        metadata['dx'] = smdx
        metadata['dy'] = smdy
        oc.addData(sx_out_soil.vs30, 'vs30', metadata=metadata)
        if 'PGA' in outsd:
            oc.addData(sd_rat, 'URATPGA', metadata=metadata)
    else:
        metadata['type'] = 'points'
        metadata['lons'] = lons.flatten()
        metadata['lats'] = lats.flatten()
        metadata['facility_ids'] = [x.encode('ascii') for x in idents]
        oc.addData(vs30.flatten(), 'vs30', metadata=metadata)

    for key, value in outgrid.items():
        oc.addData(value, key, metadata=metadata)
        oc.addData(outsd[key], key + '_sd', metadata=metadata)

    oc.close()
    if verbose: print('done')
    #------------------------------------------------------------------
    # End interp()
    #------------------------------------------------------------------

#%%
if __name__ == '__main__':
    description = '''Process a shakemap...
The only argument is a ShakeMap event ID, which should correspond to a
directory in the ShakeMap data directory of the current profile.
'''
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('eventid',
                    help='Path to ShakeMap data directory containing '
                         'input and config files.')
    parser.add_argument('-v','--verbose', action='store_true',
                        help='Print informational messages.')
    pargs = parser.parse_args()
    interp(pargs)


