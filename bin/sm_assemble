#!/usr/bin/env python
"""
Collect configuration, station data, finite fault data, etc., into
an InputContainer and write it out as shake_data.hdf.
"""

# stdlib imports
import sys
import os.path
import argparse
import glob
import datetime

# third party imports
from configobj import ConfigObj

from shakelib.utils.containers import InputContainer
from shakemap.utils.config import get_config_paths, get_custom_validator,\
        config_error, check_config, get_configspec

def get_parser():
    description = '''
    Assemble ShakeMap input data into an HDF package. The only argument is a
    ShakeMap event ID, which should correspond to a directory in the current
    profile's ShakeMap data directory. The output will be an HDF5 data file
    called shake_data.hdf in that same event's 'current' data directory.
    '''
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('eventid', 
                        help='ID of the event to process')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Print informational messages.')
    return parser

# %%
#args = type('Dummy', (object,), {'eventid': 'wenchuan',
#                                 'verbose': True})
#if True:
def main(args):
    verbose = args.verbose

    install_path, data_path = get_config_paths()
    datadir = os.path.join(data_path, args.eventid, 'current')
    if not os.path.isdir(datadir):
        print('%s is not a valid directory.' % datadir)
        sys.exit(1)

    eventxml = os.path.join(datadir, 'event.xml')
    if verbose: print('Looking for event.xml file...')
    if not os.path.isfile(eventxml):
        print('%s does not exist.' % eventxml)
        sys.exit(1)

    #
    # Look for global configs in install_path/config
    #
    spec_file = get_configspec()
    validator = get_custom_validator()
    if verbose: print('Looking for configuration files...')
    modules = ConfigObj(
            os.path.join(install_path, 'config', 'modules.conf'),
            configspec=spec_file)
    gmpe_sets = ConfigObj(
            os.path.join(install_path, 'config', 'gmpe_sets.conf'),
            configspec=spec_file)
    global_config = ConfigObj(
            os.path.join(install_path, 'config', 'model.conf'),
            configspec=spec_file)
    
    #
    # this is the event specific model.conf (may not be present)
    # prefer model.conf to model_zc.conf
    #
    event_config_file = os.path.join(datadir, 'model.conf')
    event_config_zc_file = os.path.join(datadir, 'model_zc.conf')
    if os.path.isfile(event_config_file):
        event_config = ConfigObj(event_config_file, configspec=spec_file)
    elif os.path.isfile(event_config_zc_file):
        event_config = ConfigObj(event_config_zc_file, configspec=spec_file)
    else:
        event_config = ConfigObj()

    #
    # start merging event_config
    #
    global_config.merge(event_config)
    global_config.merge(modules)
    global_config.merge(gmpe_sets)

    results = global_config.validate(validator)
    if not isinstance(results, bool) or not results:
        config_error(global_config, results)

    check_config(global_config)

    #
    # The vs30 file may have macros in it
    #
    vs30file = global_config['data']['vs30file']
    if vs30file:
        vs30file = vs30file.replace('<INSTALL_DIR>', install_path)
        vs30file = vs30file.replace('<DATA_DIR>', data_path)
        vs30file = vs30file.replace('<EVENT_ID>', args.eventid)
        if not os.path.isfile(vs30file):
            print("vs30 file '%s' is not a valid file" % vs30file)
            sys.exit(1)
        global_config['data']['vs30file'] = vs30file
    #
    # If there is a prediction_location->file file, then we need
    # to expand any macros
    #
    if 'file' in global_config['interp']['prediction_location']:
        loc_file = global_config['interp']['prediction_location']['file']
        if loc_file and loc_file != 'None':      # Note that 'None' is a string here
            loc_file = loc_file.replace('<INSTALL_DIR>', install_path)
            loc_file = loc_file.replace('<DATA_DIR>', data_path)
            loc_file = loc_file.replace('<EVENT_ID>', args.eventid)
            if not os.path.isfile(loc_file):
                print("prediction file '%s' is not a valid file" % loc_file)
                sys.exit(1)
            global_config['interp']['prediction_location']['file'] = loc_file

    config = global_config.dict()

    if verbose: print('Looking for data files...')
    datafiles = glob.glob(os.path.join(datadir, '*_dat.xml'))
    if os.path.isfile(os.path.join(datadir, 'stationlist.xml')):
        datafiles.append(os.path.join(datadir, 'stationlist.xml'))

    if verbose: print('Looking for rupture files...')
    rupturefiles = glob.glob(os.path.join(datadir, '*_fault.txt'))
    rupturefile = None
    if len(rupturefiles):
        rupturefile = rupturefiles[0]
    #
    # Sort out the version history. Get the most recent backup file and
    # extract the existing history. Then add a new line for this run.
    #
    timestamp = datetime.datetime.utcnow().strftime('%FT%TZ')
    originator = config['system']['source_network']
    backup_dirs = sorted(
            glob.glob(os.path.join(datadir, '..', '.backup*')),
            reverse=True)
    if len(backup_dirs):
        #
        # Backup files exist so find the latest one and extract its
        # history, then add a new line that increments the version
        #
        bu_file = os.path.join(backup_dirs[0], 'shake_data.hdf')
        bu_ic = InputContainer.load(bu_file)
        history = bu_ic.getVersionHistory()
        bu_ic.close()
        version = int(
                backup_dirs[0].replace(
                        os.path.join(datadir, '..', '.backup'), ''))
        version += 1
        new_line = [timestamp, originator, version]
        history['history'].append(new_line)
    elif os.path.isfile(os.path.join(datadir, 'shake_data.hdf')):
        #
        # No backups are available, but there is an existing shake_data
        # file. Extract its history and update the timestamp and
        # source network (but leave the version alone).
        #
        bu_file = os.path.join(datadir, 'shake_data.hdf')
        bu_ic = InputContainer.load(bu_file)
        history = bu_ic.getVersionHistory()
        bu_ic.close()
        new_line = [timestamp, originator, history['history'][-1][2]]
        history['history'][-1] = new_line
    else:
        #
        # No backup and no existing file. Make this version 1
        #
        history = {'history': []}
        new_line = [timestamp, originator, 1]
        history['history'].append(new_line)

    hdf_file = os.path.join(datadir, 'shake_data.hdf')

    if verbose: print('Creating input container...')
    shake_data = InputContainer.createFromInput(hdf_file, config, eventxml,
                                     rupturefile, datafiles=datafiles,
                                     version_history=history)
    print('Created HDF5 input container in %s' % shake_data.getFileName())
# %%
    shake_data.close()

# %%
if __name__ == '__main__':
    parser = get_parser()
    pargs = parser.parse_args()
    main(pargs)
